{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:34:24.065412Z","iopub.status.busy":"2024-07-10T14:34:24.065029Z","iopub.status.idle":"2024-07-10T14:34:24.070531Z","shell.execute_reply":"2024-07-10T14:34:24.069550Z","shell.execute_reply.started":"2024-07-10T14:34:24.065384Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","dataset_path = '/kaggle/input/indonesian-traffic-violation-on-motorcycle/image_crop/image crop'\n","dataset_images_path = os.path.join(dataset_path, \"image\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:34:27.276329Z","iopub.status.busy":"2024-07-10T14:34:27.275541Z","iopub.status.idle":"2024-07-10T14:34:27.623697Z","shell.execute_reply":"2024-07-10T14:34:27.622831Z","shell.execute_reply.started":"2024-07-10T14:34:27.276287Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Jumlah gambar dalam dataset: 1127\n"]}],"source":["image_files = os.listdir(dataset_images_path)\n","num_image_files = len(image_files)\n","print(f\"Jumlah gambar dalam dataset: {num_image_files}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:34:31.433992Z","iopub.status.busy":"2024-07-10T14:34:31.433277Z","iopub.status.idle":"2024-07-10T14:34:31.825994Z","shell.execute_reply":"2024-07-10T14:34:31.825147Z","shell.execute_reply.started":"2024-07-10T14:34:31.433961Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>caption</th>\n","      <th>category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TA_00001</td>\n","      <td>pengendara motor hitam baju abu tanpa pelat no...</td>\n","      <td>tanpa pelat nomor</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TA_00002</td>\n","      <td>terdapat pengendara motor silver di paling kir...</td>\n","      <td>melanggar marka</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TA_00003</td>\n","      <td>terdapat lima pengendara motor di depan mobil ...</td>\n","      <td>melanggar marka</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TA_00004</td>\n","      <td>terdapat pengendara motor hitam baju putih sen...</td>\n","      <td>melanggar marka</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TA_00005</td>\n","      <td>pengendara motor berboncengan di paling kanan ...</td>\n","      <td>melanggar marka</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   filename                                            caption  \\\n","0  TA_00001  pengendara motor hitam baju abu tanpa pelat no...   \n","1  TA_00002  terdapat pengendara motor silver di paling kir...   \n","2  TA_00003  terdapat lima pengendara motor di depan mobil ...   \n","3  TA_00004  terdapat pengendara motor hitam baju putih sen...   \n","4  TA_00005  pengendara motor berboncengan di paling kanan ...   \n","\n","            category  \n","0  tanpa pelat nomor  \n","1    melanggar marka  \n","2    melanggar marka  \n","3    melanggar marka  \n","4    melanggar marka  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","annotation = pd.read_csv('/kaggle/input/caption-hawww/captionnewbanget.csv')\n","annotation.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:34:34.400883Z","iopub.status.busy":"2024-07-10T14:34:34.400212Z","iopub.status.idle":"2024-07-10T14:34:34.491669Z","shell.execute_reply":"2024-07-10T14:34:34.490897Z","shell.execute_reply.started":"2024-07-10T14:34:34.400843Z"},"trusted":true},"outputs":[],"source":["import re\n","\n","def get_preprocessed_caption(caption):    \n","    caption = re.sub(r'\\s+', ' ', caption)\n","    caption = caption.strip()\n","    caption = re.sub(r'[,.]', '', caption)\n","    caption = \"<start> \" + caption.lower() + \" <end>\" \n","    return caption\n","\n","images_captions_dict = {}\n","\n","for idx, row in annotation.iterrows():\n","    filename = row['filename']\n","    caption = row['caption']\n","    caption = get_preprocessed_caption(caption)\n","    images_captions_dict[filename] = caption\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:34:36.499563Z","iopub.status.busy":"2024-07-10T14:34:36.499208Z","iopub.status.idle":"2024-07-10T14:34:44.802745Z","shell.execute_reply":"2024-07-10T14:34:44.801815Z","shell.execute_reply.started":"2024-07-10T14:34:36.499537Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n","100%|██████████| 230M/230M [00:01<00:00, 138MB/s]  \n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([1, 2048, 1, 1])\n","tensor([[[[1.5437]],\n","\n","         [[0.0656]],\n","\n","         [[0.2514]],\n","\n","         ...,\n","\n","         [[0.1701]],\n","\n","         [[0.2219]],\n","\n","         [[0.2330]]]])\n"]}],"source":["import torch\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from PIL import Image\n","\n","resnet_model = models.resnet101(pretrained=True)\n","resnet_model.eval()\n","resnet_model = torch.nn.Sequential(*(list(resnet152_model.children())[:-1])) \n","\n","def extract_features152(image_path):\n","    img = Image.open(image_path)\n","    img = transforms.Resize(224)(img)\n","    img = transforms.ToTensor()(img)\n","    img = img[:3, :, :]\n","    img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n","    img = img.unsqueeze(0)\n","\n","    with torch.no_grad():\n","        features = resnet_model(img)\n","\n","    return features\n","\n","image_path = '/kaggle/input/indonesian-traffic-violation-on-motorcycle/image_crop/image crop/image/TA_00001.png'\n","image_features = extract_features152(image_path)\n","print(image_features.shape)\n","print(image_features)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:35:03.103595Z","iopub.status.busy":"2024-07-10T14:35:03.102670Z","iopub.status.idle":"2024-07-10T14:40:02.527304Z","shell.execute_reply":"2024-07-10T14:40:02.526332Z","shell.execute_reply.started":"2024-07-10T14:35:03.103562Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","\n","output_dir = '/kaggle/working/OutputCodeTA/image_feature152'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","for filename in os.listdir(dataset_images_path):\n","    if filename.endswith(\".png\"): \n","        image_path = os.path.join(dataset_images_path, filename)\n","        features = extract_features152(image_path)\n","        features = features.squeeze().numpy()\n","\n","        image_id = os.path.splitext(filename)[0]\n","        output_file = os.path.join(output_dir, f\"{image_id}.npz\")\n","\n","        np.savez_compressed(output_file, feat=features)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:40:11.800737Z","iopub.status.busy":"2024-07-10T14:40:11.800395Z","iopub.status.idle":"2024-07-10T14:40:12.400272Z","shell.execute_reply":"2024-07-10T14:40:12.399385Z","shell.execute_reply.started":"2024-07-10T14:40:11.800711Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","image_filenames = list(images_captions_dict.keys())\n","image_filenames_train, image_filenames_temp = train_test_split(image_filenames, test_size=0.2, random_state=3)\n","image_filenames_val, image_filenames_test = train_test_split(image_filenames_temp, test_size=0.5, random_state=3)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:40:16.080500Z","iopub.status.busy":"2024-07-10T14:40:16.079769Z","iopub.status.idle":"2024-07-10T14:40:16.085286Z","shell.execute_reply":"2024-07-10T14:40:16.084415Z","shell.execute_reply.started":"2024-07-10T14:40:16.080472Z"},"trusted":true},"outputs":[],"source":["def remove_start_tokens(caption):\n","    start_token = \"<start>\"\n","\n","    words = caption.split()\n","  \n","    if words[0] == start_token:\n","        words = words[1:]\n","    \n","    cleaned_caption = ' '.join(words)\n","    \n","    return cleaned_caption"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:40:18.695039Z","iopub.status.busy":"2024-07-10T14:40:18.694195Z","iopub.status.idle":"2024-07-10T14:40:18.700270Z","shell.execute_reply":"2024-07-10T14:40:18.699312Z","shell.execute_reply.started":"2024-07-10T14:40:18.694997Z"},"trusted":true},"outputs":[],"source":["def remove_start_and_end_tokens(caption):\n","    start_token = \"<start>\"\n","    end_token = \"<end>\"\n","\n","    words = caption.split()\n","\n","    if words[0] == start_token:\n","        words = words[1:]\n","\n","    if words[-1] == end_token:\n","        words = words[:-1]\n","\n","    cleaned_caption = ' '.join(words)\n","\n","    return cleaned_caption"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:40:20.686074Z","iopub.status.busy":"2024-07-10T14:40:20.685619Z","iopub.status.idle":"2024-07-10T14:40:20.692097Z","shell.execute_reply":"2024-07-10T14:40:20.691037Z","shell.execute_reply.started":"2024-07-10T14:40:20.686041Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training: 901\n","Validation: 113\n","Test: 113\n"]}],"source":["print(\"Training:\", len(image_filenames_train))\n","print(\"Validation:\", len(image_filenames_val))\n","print(\"Test:\", len(image_filenames_test))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:40:23.104892Z","iopub.status.busy":"2024-07-10T14:40:23.104001Z","iopub.status.idle":"2024-07-10T14:40:23.110927Z","shell.execute_reply":"2024-07-10T14:40:23.109372Z","shell.execute_reply.started":"2024-07-10T14:40:23.104838Z"},"trusted":true},"outputs":[],"source":["label_train = []\n","\n","for image_filename in image_filenames_train:\n","    caption = images_captions_dict[image_filename]\n","    label_train.append(caption)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:40:26.724759Z","iopub.status.busy":"2024-07-10T14:40:26.723987Z","iopub.status.idle":"2024-07-10T14:40:26.731467Z","shell.execute_reply":"2024-07-10T14:40:26.730484Z","shell.execute_reply.started":"2024-07-10T14:40:26.724731Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Jumlah kata unik dalam dataset pelatihan: 96\n"]}],"source":["kata_unik = set()\n","\n","for teks in label_train:\n","    tokens = teks.split()  \n","    kata_unik.update(tokens)\n","\n","print(\"Jumlah kata unik dalam dataset pelatihan:\", len(kata_unik))"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:40:29.486243Z","iopub.status.busy":"2024-07-10T14:40:29.485887Z","iopub.status.idle":"2024-07-10T14:40:29.498460Z","shell.execute_reply":"2024-07-10T14:40:29.497556Z","shell.execute_reply.started":"2024-07-10T14:40:29.486217Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Jumlah kata unik yang disimpan dalam file teks: 96\n"]}],"source":["from collections import Counter\n","import os\n","\n","word_freq = Counter()\n","\n","for teks in label_train:\n","    tokens = teks.split()  \n","    word_freq.update(tokens)\n","\n","top_words = [word for word, _ in word_freq.most_common(96)]\n","\n","output_file = os.path.join('/kaggle/working/OutputCodeTA', 'data_vocabulary.txt')\n","with open(output_file, \"w\") as file:\n","    file.write('<unk>' + \"\\n\")\n","    for word in top_words:\n","        file.write(word + \"\\n\")\n","\n","print(\"Jumlah kata unik yang disimpan dalam file teks:\", len(top_words))"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:40:31.735697Z","iopub.status.busy":"2024-07-10T14:40:31.734982Z","iopub.status.idle":"2024-07-10T14:40:43.340335Z","shell.execute_reply":"2024-07-10T14:40:43.339314Z","shell.execute_reply.started":"2024-07-10T14:40:31.735668Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-10 14:40:33.649424: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-10 14:40:33.649539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-10 14:40:33.775140: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import tensorflow as tf\n","\n","top_k = 96\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","                                                  oov_token=\"<unk>\",\n","                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n","\n","tokenizer.fit_on_texts(label_train)\n","tokenizer.word_index['<pad>'] = 0\n","tokenizer.index_word[0] = '<pad>'\n","label_train = tokenizer.texts_to_sequences(label_train)\n","label_train = tf.keras.preprocessing.sequence.pad_sequences(label_train, padding='post')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:40:47.303233Z","iopub.status.busy":"2024-07-10T14:40:47.302605Z","iopub.status.idle":"2024-07-10T14:40:47.308625Z","shell.execute_reply":"2024-07-10T14:40:47.307703Z","shell.execute_reply.started":"2024-07-10T14:40:47.303204Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["19\n"]}],"source":["max_caption_length = max(len(t) for t in label_train)\n","print(max_caption_length)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:40:50.374649Z","iopub.status.busy":"2024-07-10T14:40:50.374290Z","iopub.status.idle":"2024-07-10T14:40:50.380295Z","shell.execute_reply":"2024-07-10T14:40:50.379298Z","shell.execute_reply.started":"2024-07-10T14:40:50.374621Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Kata dari indeks 97 adalah: 'dekat'\n"]}],"source":["index_word = tokenizer.index_word\n","\n","index = 97 \n","word = index_word[index]\n","print(f\"Kata dari indeks {index} adalah: '{word}'\")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T08:46:55.258946Z","iopub.status.busy":"2024-07-07T08:46:55.258552Z","iopub.status.idle":"2024-07-07T08:46:55.266225Z","shell.execute_reply":"2024-07-07T08:46:55.265279Z","shell.execute_reply.started":"2024-07-07T08:46:55.258916Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['<start>',\n"," 'lima',\n"," 'pengendara',\n"," 'motor',\n"," 'di',\n"," 'depan',\n"," 'mobil',\n"," 'abu',\n"," 'berhenti',\n"," 'melewati',\n"," 'garis',\n"," 'marka',\n"," '<end>',\n"," '<pad>',\n"," '<pad>',\n"," '<pad>',\n"," '<pad>',\n"," '<pad>',\n"," '<pad>']"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["[tokenizer.index_word[i] for i in label_train[23]]"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T08:43:11.590399Z","iopub.status.busy":"2024-07-10T08:43:11.589535Z","iopub.status.idle":"2024-07-10T08:43:11.596152Z","shell.execute_reply":"2024-07-10T08:43:11.595277Z","shell.execute_reply.started":"2024-07-10T08:43:11.590366Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([ 3, 75,  2,  5,  6, 16, 17, 59, 12, 14,  8,  7,  4,  0,  0,  0,  0,\n","        0,  0], dtype=int32)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["label_train[23]"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:01.416988Z","iopub.status.busy":"2024-07-10T14:41:01.416274Z","iopub.status.idle":"2024-07-10T14:41:01.458151Z","shell.execute_reply":"2024-07-10T14:41:01.457219Z","shell.execute_reply.started":"2024-07-10T14:41:01.416957Z"},"trusted":true},"outputs":[],"source":["urutan_input_dict = {}\n","\n","for image_filename in image_filenames_train:\n","    captions = []\n","    image = image_filename.split('.')[0]\n","    caption = images_captions_dict[image_filename]\n","    captions.append(caption)\n","    \n","    caption_input = tokenizer.texts_to_sequences(captions)\n","\n","    caption_input = tf.keras.preprocessing.sequence.pad_sequences(caption_input, padding='post', maxlen=max_caption_length)\n","        \n","    if image_filename not in urutan_input_dict:\n","        urutan_input_dict[image] = caption_input"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:05.831985Z","iopub.status.busy":"2024-07-10T14:41:05.831625Z","iopub.status.idle":"2024-07-10T14:41:05.841978Z","shell.execute_reply":"2024-07-10T14:41:05.841103Z","shell.execute_reply.started":"2024-07-10T14:41:05.831957Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","output_file = os.path.join('/kaggle/working/OutputCodeTA', 'urutan_input_dict.pkl')\n","with open(output_file, 'wb') as f:\n","    pickle.dump(urutan_input_dict, f)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:08.456132Z","iopub.status.busy":"2024-07-10T14:41:08.455272Z","iopub.status.idle":"2024-07-10T14:41:08.502398Z","shell.execute_reply":"2024-07-10T14:41:08.501480Z","shell.execute_reply.started":"2024-07-10T14:41:08.456099Z"},"trusted":true},"outputs":[],"source":["urutan_target_dict = {}\n","\n","for image_filename in image_filenames_train:\n","    captions = []\n","    image = image_filename.split('.')[0]\n","    caption = images_captions_dict[image_filename]\n","\n","    caption = remove_start_and_end_tokens(caption)\n","    captions.append(caption)\n","    \n","    caption_target = tokenizer.texts_to_sequences(captions)\n","\n","    caption_target = tf.keras.preprocessing.sequence.pad_sequences(caption_target, padding='post', maxlen=max_caption_length)\n","    \n","    if image_filename not in urutan_target_dict:\n","        urutan_target_dict[image] = caption_target"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:10.842331Z","iopub.status.busy":"2024-07-10T14:41:10.841993Z","iopub.status.idle":"2024-07-10T14:41:10.852139Z","shell.execute_reply":"2024-07-10T14:41:10.850738Z","shell.execute_reply.started":"2024-07-10T14:41:10.842306Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","output_file = os.path.join('/kaggle/working/OutputCodeTA', 'urutan_target_dict.pkl')\n","with open(output_file, 'wb') as f:\n","    pickle.dump(urutan_target_dict, f)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:13.635319Z","iopub.status.busy":"2024-07-10T14:41:13.634417Z","iopub.status.idle":"2024-07-10T14:41:13.647509Z","shell.execute_reply":"2024-07-10T14:41:13.646545Z","shell.execute_reply.started":"2024-07-10T14:41:13.635269Z"},"trusted":true},"outputs":[],"source":["urutan_target_dictval = {}\n","\n","for image_filename in image_filenames_val:\n","    captions = []\n","    image = image_filename.split('.')[0]\n","    caption = images_captions_dict[image_filename]\n","\n","    caption = remove_start_and_end_tokens(caption)\n","    captions.append(caption)\n","    \n","    caption_target = tokenizer.texts_to_sequences(captions)\n","\n","    caption_target = tf.keras.preprocessing.sequence.pad_sequences(caption_target, padding='post', maxlen=max_caption_length)\n","    \n","    if image_filename not in urutan_target_dictval:\n","        urutan_target_dictval[image] = caption_target"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:15.747797Z","iopub.status.busy":"2024-07-10T14:41:15.747445Z","iopub.status.idle":"2024-07-10T14:41:15.753781Z","shell.execute_reply":"2024-07-10T14:41:15.752779Z","shell.execute_reply.started":"2024-07-10T14:41:15.747769Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","output_file = os.path.join('/kaggle/working/OutputCodeTA', 'urutan_target_dictval.pkl')\n","with open(output_file, 'wb') as f:\n","    pickle.dump(urutan_target_dictval, f)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:18.680093Z","iopub.status.busy":"2024-07-10T14:41:18.679291Z","iopub.status.idle":"2024-07-10T14:41:18.687125Z","shell.execute_reply":"2024-07-10T14:41:18.686134Z","shell.execute_reply.started":"2024-07-10T14:41:18.680055Z"},"trusted":true},"outputs":[],"source":["output_file = os.path.join('/kaggle/working/OutputCodeTA', 'image_filenames_train_id.txt')\n","with open(output_file, \"w\") as file:\n","    for image_filename in image_filenames_train:\n","        image_filename = image_filename.split('.')[0]\n","        file.write(image_filename + \"\\n\")"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2024-07-11T10:21:39.661246Z","iopub.status.busy":"2024-07-11T10:21:39.660564Z","iopub.status.idle":"2024-07-11T10:21:39.667200Z","shell.execute_reply":"2024-07-11T10:21:39.666257Z","shell.execute_reply.started":"2024-07-11T10:21:39.661206Z"},"trusted":true},"outputs":[],"source":["output_file = os.path.join('/kaggle/working/OutputCodeTA', 'image_filenames_trainval_id.txt')\n","with open(output_file, \"w\") as file:\n","    i = 0\n","    for image_filename in image_filenames_train:\n","        image_filename = image_filename.split('.')[0]\n","        file.write(image_filename + \"\\n\")\n","        if i == 100:\n","            break\n","        i += 1"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:20.982196Z","iopub.status.busy":"2024-07-10T14:41:20.981498Z","iopub.status.idle":"2024-07-10T14:41:20.987246Z","shell.execute_reply":"2024-07-10T14:41:20.986384Z","shell.execute_reply.started":"2024-07-10T14:41:20.982167Z"},"trusted":true},"outputs":[],"source":["output_file = os.path.join('/kaggle/working/OutputCodeTA', 'image_filenames_val_id.txt')\n","with open(output_file, \"w\") as file:\n","    for image_filename in image_filenames_val:\n","        image_filename = image_filename.split('.')[0]\n","        file.write(image_filename + \"\\n\")"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:23.344983Z","iopub.status.busy":"2024-07-10T14:41:23.344456Z","iopub.status.idle":"2024-07-10T14:41:23.350490Z","shell.execute_reply":"2024-07-10T14:41:23.349504Z","shell.execute_reply.started":"2024-07-10T14:41:23.344939Z"},"trusted":true},"outputs":[],"source":["output_file = os.path.join('/kaggle/working/OutputCodeTA', 'image_filenames_test_id.txt')\n","with open(output_file, \"w\") as file:\n","    for image_filename in image_filenames_test:\n","        image_filename = image_filename.split('.')[0]\n","        file.write(image_filename + \"\\n\")"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2024-07-11T10:22:05.719419Z","iopub.status.busy":"2024-07-11T10:22:05.719056Z","iopub.status.idle":"2024-07-11T10:22:05.756665Z","shell.execute_reply":"2024-07-11T10:22:05.755727Z","shell.execute_reply.started":"2024-07-11T10:22:05.719390Z"},"trusted":true},"outputs":[],"source":["import os\n","import os.path as osp\n","import numpy as np\n","from easydict import EasyDict as edict\n","\n","__C = edict()\n","\n","cfg = __C\n","\n","# ---------------------------------------------------------------------------- #\n","# Training options\n","# ---------------------------------------------------------------------------- #\n","__C.TRAIN = edict()\n","\n","# Minibatch size\n","__C.TRAIN.BATCH_SIZE = 10\n","\n","# scheduled sampling\n","__C.TRAIN.SCHEDULED_SAMPLING = edict()\n","\n","__C.TRAIN.SCHEDULED_SAMPLING.START = 6\n","\n","__C.TRAIN.SCHEDULED_SAMPLING.INC_EVERY = 5\n","\n","__C.TRAIN.SCHEDULED_SAMPLING.INC_PROB = 0.05\n","\n","__C.TRAIN.SCHEDULED_SAMPLING.MAX_PROB = 0.5\n","\n","# ---------------------------------------------------------------------------- #\n","# Inference ('test') options\n","# ---------------------------------------------------------------------------- #\n","__C.TEST = edict()\n","\n","# Minibatch size\n","__C.TEST.BATCH_SIZE = 16\n","\n","\n","# ---------------------------------------------------------------------------- #\n","# Data loader options\n","# ---------------------------------------------------------------------------- #\n","__C.DATA_LOADER = edict()\n","\n","# Data directory\n","__C.DATA_LOADER.NUM_WORKERS = 3\n","\n","__C.DATA_LOADER.PIN_MEMORY = True\n","\n","__C.DATA_LOADER.DROP_LAST = True\n","\n","__C.DATA_LOADER.SHUFFLE = True\n","\n","__C.DATA_LOADER.TRAIN_GV_FEAT = ''\n","\n","__C.DATA_LOADER.TRAIN_ATT_FEATS = '/kaggle/working/OutputCodeTA/image_feature152'\n","\n","__C.DATA_LOADER.VAL_GV_FEAT = ''\n","\n","__C.DATA_LOADER.VAL_ATT_FEATS = '/kaggle/working/OutputCodeTA/image_feature152'\n","\n","__C.DATA_LOADER.TEST_GV_FEAT = ''\n","\n","__C.DATA_LOADER.TEST_ATT_FEATS = '/kaggle/working/OutputCodeTA/image_feature152'\n","\n","__C.DATA_LOADER.TRAIN_ID = '/kaggle/working/OutputCodeTA/image_filenames_train_id.txt'\n","\n","__C.DATA_LOADER.VALT_ID = '/kaggle/working/OutputCodeTA/image_filenames_trainval_id.txt'\n","\n","__C.DATA_LOADER.VAL_ID = '/kaggle/working/OutputCodeTA/image_filenames_val_id.txt'\n","\n","__C.DATA_LOADER.TEST_ID = '/kaggle/working/OutputCodeTA/image_filenames_test_id.txt'\n","\n","__C.DATA_LOADER.INPUT_SEQ_PATH = '/kaggle/working/OutputCodeTA/urutan_input_dict.pkl'\n","\n","__C.DATA_LOADER.TARGET_SEQ_PATH = '/kaggle/working/OutputCodeTA/urutan_target_dict.pkl'\n","\n","__C.DATA_LOADER.TARGETVAL_SEQ_PATH = '/kaggle/working/OutputCodeTA/urutan_target_dictval.pkl'\n","\n","__C.DATA_LOADER.SEQ_PER_IMG = 1\n","\n","__C.DATA_LOADER.MAX_FEAT = -1\n","\n","# ---------------------------------------------------------------------------- #\n","# Model options\n","# ---------------------------------------------------------------------------- #\n","__C.MODEL = edict()\n","\n","__C.MODEL.TYPE = 'XLAN'            \n","\n","__C.MODEL.SEQ_LEN = 19             \n","\n","__C.MODEL.VOCAB_SIZE = 96        \n","\n","__C.MODEL.WORD_EMBED_DIM = 256 #1024\n","\n","__C.MODEL.WORD_EMBED_ACT = 'CELU'       \n","\n","__C.MODEL.WORD_EMBED_NORM = False\n","\n","__C.MODEL.DROPOUT_WORD_EMBED = 0.5 #0.5\n","\n","__C.MODEL.GVFEAT_DIM = 2048\n","\n","__C.MODEL.GVFEAT_EMBED_DIM = -1\n","\n","__C.MODEL.GVFEAT_EMBED_ACT = 'NONE'     \n","\n","__C.MODEL.DROPOUT_GV_EMBED = 0.0\n","\n","__C.MODEL.ATT_FEATS_DIM = 2048\n","\n","__C.MODEL.ATT_FEATS_EMBED_DIM = 1024\n","\n","__C.MODEL.ATT_FEATS_EMBED_ACT = 'CELU'   \n","\n","__C.MODEL.DROPOUT_ATT_EMBED = 0.5\n","\n","__C.MODEL.ATT_FEATS_NORM = False\n","\n","__C.MODEL.ATT_HIDDEN_SIZE = -1\n","\n","__C.MODEL.ATT_HIDDEN_DROP = 0.0\n","\n","__C.MODEL.ATT_ACT = 'TANH'  \n","\n","__C.MODEL.RNN_SIZE = 1024\n","\n","__C.MODEL.DROPOUT_LM = 0.5 \n","\n","\n","# Bilinear\n","__C.MODEL.BILINEAR = edict()\n","\n","__C.MODEL.BILINEAR.DIM = 1024\n","\n","__C.MODEL.BILINEAR.DECODE_DIM = 1024\n","\n","__C.MODEL.BILINEAR.ENCODE_ATT_MID_DIM = [128, 64, 128]\n","\n","__C.MODEL.BILINEAR.DECODE_ATT_MID_DIM = [128, 64, 128]\n","\n","__C.MODEL.BILINEAR.ENCODE_ATT_MID_DROPOUT = 0.1\n","\n","__C.MODEL.BILINEAR.DECODE_ATT_MID_DROPOUT = 0.1\n","\n","__C.MODEL.BILINEAR.ATT_DIM = 1024\n","\n","__C.MODEL.BILINEAR.ACT = 'CELU'  \n","\n","__C.MODEL.BILINEAR.ENCODE_DROPOUT = 0.5\n","\n","__C.MODEL.BILINEAR.DECODE_DROPOUT = 0.5 \n","\n","__C.MODEL.BILINEAR.ENCODE_LAYERS = 4        # Orde 8, ubah jumlah encode layer jika ingin mengganti orde\n","\n","__C.MODEL.BILINEAR.DECODE_LAYERS = 1\n","\n","__C.MODEL.BILINEAR.TYPE = 'LowRank'\n","\n","__C.MODEL.BILINEAR.ATTTYPE = 'SCAtt'\n","\n","__C.MODEL.BILINEAR.HEAD = 8\n","\n","__C.MODEL.BILINEAR.DECODE_HEAD = 8\n","\n","__C.MODEL.BILINEAR.ENCODE_FF_DROPOUT = 0.1\n","\n","__C.MODEL.BILINEAR.DECODE_FF_DROPOUT = 0.1\n","\n","__C.MODEL.BILINEAR.ENCODE_BLOCK = 'LowRankBilinearEnc'\n","\n","__C.MODEL.BILINEAR.DECODE_BLOCK = 'LowRankBilinearDec'\n","\n","__C.MODEL.BILINEAR.ELU_ALPHA = 1.3\n","\n","__C.MODEL.BILINEAR.BIFEAT_EMB_ACT = 'RELU'\n","\n","__C.MODEL.BILINEAR.ENCODE_BIFEAT_EMB_DROPOUT = 0.3\n","\n","__C.MODEL.BILINEAR.DECODE_BIFEAT_EMB_DROPOUT = 0.3\n","\n","# ---------------------------------------------------------------------------- #\n","# Solver options\n","# ---------------------------------------------------------------------------- #\n","__C.SOLVER = edict()\n","\n","# Base learning rate for the specified schedule\n","__C.SOLVER.BASE_LR = 0.00004\n","\n","# Solver type\n","__C.SOLVER.TYPE = 'ADAM'                 # 'ADAM', 'ADAMAX', 'SGD'\n","\n","# Maximum number of SGD iterations\n","__C.SOLVER.MAX_EPOCH = 70\n","\n","__C.SOLVER.MAX_ITER = -1\n","\n","\n","# L2 regularization hyperparameter\n","__C.SOLVER.WEIGHT_DECAY = 0.0001\n","\n","__C.SOLVER.WEIGHT_DECAY_BIAS = 0.0\n","\n","__C.SOLVER.BIAS_LR_FACTOR = 1\n","\n","__C.SOLVER.DISPLAY = 20\n","\n","__C.SOLVER.TEST_INTERVAL = 1\n","\n","__C.SOLVER.SNAPSHOT_ITERS = 3\n","\n","# SGD\n","__C.SOLVER.SGD = edict()\n","__C.SOLVER.SGD.MOMENTUM = 0.95\n","\n","# ADAM\n","__C.SOLVER.ADAM = edict()\n","__C.SOLVER.ADAM.BETAS = [0.9, 0.999]\n","__C.SOLVER.ADAM.EPS = 1e-8\n","\n","# ---------------------------------------------------------------------------- #\n","# Losses options\n","# ---------------------------------------------------------------------------- #\n","__C.LOSSES = edict()\n","\n","__C.LOSSES.XE_TYPE = 'CrossEntropy'   \n","\n","\n","# ---------------------------------------------------------------------------- #\n","# PARAM options\n","# ---------------------------------------------------------------------------- #\n","__C.PARAM = edict()\n","\n","__C.PARAM.WT = 'WT'\n","\n","__C.PARAM.GLOBAL_FEAT = 'GV_FEAT'\n","\n","__C.PARAM.ATT_FEATS = 'ATT_FEATS'\n","\n","__C.PARAM.ATT_FEATS_MASK = 'ATT_FEATS_MASK'\n","\n","__C.PARAM.P_ATT_FEATS = 'P_ATT_FEATS'\n","\n","__C.PARAM.STATE = 'STATE'\n","\n","__C.PARAM.INPUT_SENT = 'INPUT_SENT'\n","\n","__C.PARAM.TARGET_SENT = 'TARGET_SENT'\n","\n","__C.PARAM.INDICES = 'INDICES'\n","\n","# ---------------------------------------------------------------------------- #\n","# Inference options\n","# ---------------------------------------------------------------------------- #\n","__C.INFERENCE = edict()\n","\n","__C.INFERENCE.VOCAB = '/kaggle/working/OutputCodeTA/data_vocabulary.txt'\n","\n","__C.INFERENCE.BEAM_SIZE = 3\n","\n","__C.INFERENCE.GREEDY_DECODE = True # Greedy decode or sample decode\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:29.400006Z","iopub.status.busy":"2024-07-10T14:41:29.399631Z","iopub.status.idle":"2024-07-10T14:41:29.415758Z","shell.execute_reply":"2024-07-10T14:41:29.414678Z","shell.execute_reply.started":"2024-07-10T14:41:29.399976Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","\n","def activation(act):\n","    if act == 'RELU':\n","        return nn.ReLU()\n","    elif act == 'TANH':\n","        return nn.Tanh()\n","    elif act == 'GLU':\n","        return nn.GLU()\n","    elif act == 'ELU':\n","        return nn.ELU(cfg.MODEL.BILINEAR.ELU_ALPHA)\n","    elif act == 'CELU':\n","        return nn.CELU(cfg.MODEL.BILINEAR.ELU_ALPHA)\n","    else:\n","        return nn.Identity()\n","\n","def expand_tensor(tensor, size, dim=1):\n","    if size == 1 or tensor is None:\n","        return tensor\n","    tensor = tensor.unsqueeze(dim)\n","    tensor = tensor.expand(list(tensor.shape[:dim]) + [size] + list(tensor.shape[dim+1:])).contiguous()\n","    tensor = tensor.view(list(tensor.shape[:dim-1]) + [-1] + list(tensor.shape[dim+1:]))\n","    return tensor\n","\n","def load_ids(path):\n","    with open(path, 'r') as fid:\n","        lines = [line.strip() for line in fid]\n","    return lines\n","\n","def load_lines(path):\n","    with open(path, 'r') as fid:\n","        lines = [line.strip() for line in fid]\n","    return lines\n","\n","def load_vocab(path):\n","    vocab = ['.']\n","    with open(path, 'r') as fid:\n","        for line in fid:\n","            vocab.append(line.strip())\n","    return vocab\n","\n","def decode_sequence(vocab, seq):\n","    N, T = seq.size()\n","    sents = []\n","    for n in range(N):\n","        words = []\n","        for t in range(T):\n","            ix = seq[n, t]\n","            if ix == 0:\n","                break\n","            if ix == 4:\n","                break\n","            words.append(vocab[ix])\n","        sent = ' '.join(words)\n","        sents.append(sent)\n","    return sents\n","\n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-07-11T08:36:05.402918Z","iopub.status.busy":"2024-07-11T08:36:05.402544Z","iopub.status.idle":"2024-07-11T08:36:05.418092Z","shell.execute_reply":"2024-07-11T08:36:05.416971Z","shell.execute_reply.started":"2024-07-11T08:36:05.402888Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class Optimizer(nn.Module):\n","    def __init__(self, model):\n","        super(Optimizer, self).__init__()\n","        self.setup_optimizer(model)\n","\n","    def setup_optimizer(self, model):\n","        params = []\n","        for key, value in model.named_parameters():\n","            if not value.requires_grad:\n","                continue\n","            lr = cfg.SOLVER.BASE_LR\n","            weight_decay = cfg.SOLVER.WEIGHT_DECAY\n","            if \"bias\" in key:\n","                lr = cfg.SOLVER.BASE_LR * cfg.SOLVER.BIAS_LR_FACTOR \n","                weight_decay = cfg.SOLVER.WEIGHT_DECAY_BIAS\n","            params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay}]\n","\n","        if cfg.SOLVER.TYPE == 'SGD':\n","            self.optimizer = torch.optim.SGD(\n","                params, \n","                lr = cfg.SOLVER.BASE_LR, \n","                momentum = cfg.SOLVER.SGD.MOMENTUM\n","            )\n","        elif cfg.SOLVER.TYPE == 'ADAM':\n","            self.optimizer = torch.optim.Adam(\n","                params,\n","                lr = cfg.SOLVER.BASE_LR, \n","                betas = cfg.SOLVER.ADAM.BETAS, \n","                eps = cfg.SOLVER.ADAM.EPS\n","            )\n","        elif cfg.SOLVER.TYPE == 'ADAMAX':\n","            self.optimizer = torch.optim.Adamax(\n","                params,\n","                lr = cfg.SOLVER.BASE_LR, \n","                betas = cfg.SOLVER.ADAM.BETAS, \n","                eps = cfg.SOLVER.ADAM.EPS\n","            )\n","        else:\n","            raise NotImplementedError\n","            \n","        \n","        return self.optimizer\n","            \n","    def zero_grad(self):\n","        self.optimizer.zero_grad()\n","\n","    def step(self):\n","        self.optimizer.step()\n","    \n","    def scheduler1(self):\n","        return torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.8)\n","    \n","    def scheduler2(self):\n","        return torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[4,15,25], gamma=0.1)\n","    \n","    def scheduler3(self):\n","        return torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n","\n","    def get_lr(self):\n","        lr = []\n","        for param_group in self.optimizer.param_groups:\n","            lr.append(param_group['lr'])\n","        lr = sorted(list(set(lr)))\n","        return lr"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2024-07-11T10:13:44.384125Z","iopub.status.busy":"2024-07-11T10:13:44.383669Z","iopub.status.idle":"2024-07-11T10:13:44.542038Z","shell.execute_reply":"2024-07-11T10:13:44.541245Z","shell.execute_reply.started":"2024-07-11T10:13:44.384086Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from functools import reduce\n","\n","class BasicAtt(nn.Module):\n","    def __init__(self, mid_dims, mid_dropout):\n","        super(BasicAtt, self).__init__()\n","        sequential = []\n","        for i in range(1, len(mid_dims) - 1):\n","            sequential.append(nn.Linear(mid_dims[i - 1], mid_dims[i]))\n","            sequential.append(nn.ReLU())\n","            if mid_dropout > 0:\n","                sequential.append(nn.Dropout(mid_dropout))\n","        self.attention_basic = nn.Sequential(*sequential) if len(sequential) > 0 else None\n","        self.attention_last = nn.Linear(mid_dims[-2], mid_dims[-1])\n","\n","    def forward(self, att_map, att_mask, value1, value2):\n","        if self.attention_basic is not None:\n","            att_map = self.attention_basic(att_map)\n","        attn_weights = self.attention_last(att_map)\n","        attn_weights = attn_weights.squeeze(-1)\n","        if att_mask is not None:\n","            attn_weights = attn_weights.masked_fill(att_mask.unsqueeze(1) == 0, -1e9)\n","        attn_weights = F.softmax(attn_weights, dim=-1)\n","        \n","        attn = torch.matmul(attn_weights.unsqueeze(-2), value2).squeeze(-2)\n","        return attn\n","    \n","class SCAtt(BasicAtt):\n","    def __init__(self, mid_dims, mid_dropout):\n","        super(SCAtt, self).__init__(mid_dims, mid_dropout)\n","        self.attention_last = nn.Linear(mid_dims[-2], 1)\n","        self.attention_last2 = nn.Linear(mid_dims[-2], mid_dims[-1])\n","\n","    def forward(self, att_map, att_mask, value1, value2):\n","        if self.attention_basic is not None:\n","            att_map = self.attention_basic(att_map)\n","\n","        if att_mask is not None:\n","            att_mask = att_mask.unsqueeze(1)\n","            att_mask_ext = att_mask.unsqueeze(-1)\n","            att_map_pool = torch.sum(att_map * att_mask_ext, -2) / torch.sum(att_mask_ext, -2)\n","        else:\n","            att_map_pool = att_map.mean(-2)\n","\n","        alpha_spatial = self.attention_last(att_map)\n","        alpha_channel = self.attention_last2(att_map_pool)\n","        alpha_channel = torch.sigmoid(alpha_channel)\n","\n","        alpha_spatial = alpha_spatial.squeeze(-1)\n","        if att_mask is not None:\n","            alpha_spatial = alpha_spatial.masked_fill(att_mask == 0, -1e9)\n","        alpha_spatial = F.softmax(alpha_spatial, dim=-1)\n","\n","        if len(alpha_spatial.shape) == 4: # batch_size * head_num * seq_num * seq_num (for xtransformer)\n","            value2 = torch.matmul(alpha_spatial, value2)\n","        else:\n","            value2 = torch.matmul(alpha_spatial.unsqueeze(-2), value2).squeeze(-2)\n","\n","        attn = value1 * value2 * alpha_channel\n","\n","        return attn\n","    \n","class LowRank(nn.Module):\n","    def __init__(self, embed_dim, att_type, att_heads, att_mid_dim, att_mid_drop):\n","        super(LowRank, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = att_heads\n","        self.head_dim = embed_dim // self.num_heads\n","        self.scaling = self.head_dim ** -0.5\n","        output_dim = 2 * embed_dim if cfg.MODEL.BILINEAR.ACT == 'GLU' else embed_dim #CELU\n","\n","        sequential = []\n","        sequential.append(nn.Linear(embed_dim, output_dim))\n","        act = activation(cfg.MODEL.BILINEAR.ACT)\n","        if act is not None:\n","            sequential.append(act)\n","        sequential.append(torch.nn.GroupNorm(self.num_heads, embed_dim))\n","        self.in_proj_q = nn.Sequential(*sequential)\n","\n","        sequential = []\n","        sequential.append(nn.Linear(embed_dim, output_dim))\n","        act = activation(cfg.MODEL.BILINEAR.ACT)\n","        if act is not None:\n","            sequential.append(act)\n","        sequential.append(torch.nn.GroupNorm(self.num_heads, embed_dim))\n","        self.in_proj_k = nn.Sequential(*sequential)\n","        \n","        sequential = []\n","        sequential.append(nn.Linear(embed_dim, output_dim))\n","        act = activation(cfg.MODEL.BILINEAR.ACT)\n","        if act is not None:\n","            sequential.append(act)\n","        sequential.append(torch.nn.GroupNorm(self.num_heads, embed_dim))\n","        self.in_proj_v1 = nn.Sequential(*sequential)\n","\n","        sequential = []\n","        sequential.append(nn.Linear(embed_dim, output_dim))\n","        act = activation(cfg.MODEL.BILINEAR.ACT)\n","        if act is not None:\n","            sequential.append(act)\n","        sequential.append(torch.nn.GroupNorm(self.num_heads, embed_dim))\n","        self.in_proj_v2 = nn.Sequential(*sequential)\n","\n","        self.attn_net = create_layer(att_type, att_mid_dim, att_mid_drop)\n","        self.clear_buffer() \n","\n","    def apply_to_states(self, fn):\n","        self.buffer_keys = fn(self.buffer_keys)\n","        self.buffer_value2 = fn(self.buffer_value2)\n","\n","    def init_buffer(self, batch_size):\n","        self.buffer_keys = torch.zeros((batch_size, self.num_heads, 0, self.head_dim))\n","        self.buffer_value2 = torch.zeros((batch_size, self.num_heads, 0, self.head_dim))\n","\n","    def clear_buffer(self):\n","        self.buffer_keys = None\n","        self.buffer_value2 = None\n","\n","    def forward(self, query, key, mask, value1, value2, precompute=False):\n","        batch_size = query.size()[0]\n","        q = self.in_proj_q(query)\n","        v1 = self.in_proj_v1(value1)\n","\n","        q = q.view(batch_size, self.num_heads, self.head_dim)\n","        v1 = v1.view(batch_size, self.num_heads, self.head_dim)\n","\n","        if precompute == False:\n","            key = key.view(-1, key.size()[-1])\n","            value2 = value2.view(-1, value2.size()[-1])\n","            k = self.in_proj_k(key)\n","            v2 = self.in_proj_v2(value2)\n","            \n","            k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","            v2 = v2.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","        else:\n","            k = key\n","            v2 = value2\n","\n","        attn_map = q.unsqueeze(-2) * k\n","        attn = self.attn_net(attn_map, mask, v1, v2)\n","        attn = attn.view(batch_size, self.num_heads * self.head_dim)\n","        return attn\n","\n","    def forward2(self, query, key, mask, value1, value2, precompute=False):\n","        batch_size = query.size()[0]\n","        query = query.view(-1, query.size()[-1])\n","        value1 = value1.view(-1, value1.size()[-1])\n","        \n","        q = self.in_proj_q(query)\n","        v1 = self.in_proj_v1(value1)\n","\n","        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","        v1 = v1.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","        if precompute == False:\n","            key = key.view(-1, key.size()[-1])\n","            value2 = value2.view(-1, value2.size()[-1])\n","            k = self.in_proj_k(key)\n","            v2 = self.in_proj_v2(value2)\n","            k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","            v2 = v2.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","            if self.buffer_keys is not None and self.buffer_value2 is not None:\n","                self.buffer_keys = torch.cat([self.buffer_keys, k], dim=2)\n","                self.buffer_value2 = torch.cat([self.buffer_value2, v2], dim=2)\n","                k = self.buffer_keys\n","                v2 = self.buffer_value2\n","        else:\n","            k = key\n","            v2 = value2\n","        \n","        attn_map = q.unsqueeze(-2) * k.unsqueeze(-3)\n","        attn = self.attn_net.forward(attn_map, mask, v1, v2).transpose(1, 2).contiguous()\n","        attn = attn.view(batch_size, -1, self.num_heads * self.head_dim)\n","        return attn\n","\n","    def precompute(self, key, value2):\n","        batch_size = value2.size()[0]\n","        key = key.view(-1, key.size()[-1])\n","        value2 = value2.view(-1, value2.size()[-1])\n","    \n","        k = self.in_proj_k(key)\n","        \n","        v2 = self.in_proj_v2(value2)\n","\n","        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","        v2 = v2.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","\n","        return k, v2\n","    \n","__factory_layer = {\n","    'LowRank': LowRank,\n","    'BasicAtt': BasicAtt,\n","    'SCAtt': SCAtt,\n","}\n","\n","def names_layer():\n","    return sorted(__factory_layer.keys())\n","\n","def create_layer(name, *args, **kwargs):\n","    if name not in __factory_layer:\n","        raise KeyError(\"Unknown layer:\", name)\n","    return __factory_layer[name](*args, **kwargs)\n","\n","\n","class LowRankBilinearLayer(nn.Module):\n","    def __init__(self, embed_dim, att_type, att_heads,\n","        att_mid_dim, att_mid_drop, dropout):\n","        super(LowRankBilinearLayer, self).__init__()\n","        self.encoder_attn = LowRank(\n","            embed_dim = embed_dim, \n","            att_type = att_type, \n","            att_heads = att_heads, \n","            att_mid_dim = att_mid_dim, \n","            att_mid_drop = att_mid_drop\n","        )\n","        self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n","\n","    def forward(self, x, key=None, mask=None, \n","        value1=None, value2=None, precompute=False):    \n","        x = self.encoder_attn(\n","            query=x,\n","            key=key if key is not None else x,\n","            mask=mask,\n","            value1=value1 if value1 is not None else x,\n","            value2=value2 if value2 is not None else x,\n","            precompute=precompute\n","        )\n","        if self.dropout is not None:\n","            x = self.dropout(x)\n","        return x\n","\n","    def precompute(self, key, value2):\n","        return self.encoder_attn.precompute(key, value2)\n","\n","class LowRankBilinearEncBlock(nn.Module):\n","    def __init__(self, embed_dim, att_type, att_heads, att_mid_dim,\n","        att_mid_drop, dropout, layer_num):\n","        super(LowRankBilinearEncBlock, self).__init__()\n","        \n","        self.layers = nn.ModuleList([])\n","        self.bifeat_emb = nn.ModuleList([])\n","        self.layer_norms = nn.ModuleList([]) \n","        for _ in range(layer_num):\n","            sublayer = LowRankBilinearLayer( \n","                embed_dim = embed_dim, \n","                att_type = att_type,\n","                att_heads = att_heads,\n","                att_mid_dim = att_mid_dim,\n","                att_mid_drop = att_mid_drop,\n","                dropout = dropout\n","            )\n","            self.layers.append(sublayer)\n","\n","            self.bifeat_emb.append(nn.Sequential(\n","                nn.Linear(2 * embed_dim, embed_dim),\n","                activation(cfg.MODEL.BILINEAR.BIFEAT_EMB_ACT),\n","                nn.Dropout(cfg.MODEL.BILINEAR.ENCODE_BIFEAT_EMB_DROPOUT)\n","            ))\n","\n","            self.layer_norms.append(torch.nn.LayerNorm(embed_dim))\n","        \n","        self.projj = nn.Linear(embed_dim, cfg.MODEL.BILINEAR.DECODE_DIM)\n","        self.proj = nn.Linear(embed_dim * (layer_num + 1), cfg.MODEL.BILINEAR.DECODE_DIM)\n","        self.layer_norm = torch.nn.LayerNorm(cfg.MODEL.BILINEAR.DECODE_DIM)\n","        \n","    def forward(self, gv_feat, att_feats, att_mask, p_att_feats=None):\n","        if gv_feat.shape[-1] == 1:  # empty gv_feat\n","            gv_feat = torch.sum(att_feats * att_mask.unsqueeze(-1), 1) / torch.sum(att_mask.unsqueeze(-1), 1)\n","        \n","        feat_arr = [gv_feat]\n","        for i, layer in enumerate(self.layers):\n","            gv_feat = layer(gv_feat, att_feats, att_mask, gv_feat, att_feats)\n","            att_feats_cat = torch.cat([gv_feat.unsqueeze(1).expand_as(att_feats), att_feats], dim = -1)\n","            att_feats = self.bifeat_emb[i](att_feats_cat) + att_feats\n","            att_feats = self.layer_norms[i](att_feats)\n","            feat_arr.append(gv_feat)\n","            \n","        att_feats = self.projj(att_feats)\n","        gv_feat = torch.cat(feat_arr, dim=-1)\n","        gv_feat = self.proj(gv_feat)\n","        gv_feat = self.layer_norm(gv_feat)\n","   \n","        return gv_feat, att_feats\n","\n","class LowRankBilinearDecBlock(nn.Module):\n","    def __init__(self, embed_dim, att_type, att_heads,\n","        att_mid_dim, att_mid_drop, dropout, layer_num):\n","        super(LowRankBilinearDecBlock, self).__init__()\n","        \n","        self.layers = nn.ModuleList([])\n","        for _ in range(layer_num):\n","            sublayer = LowRankBilinearLayer( \n","                embed_dim = embed_dim, \n","                att_type = att_type,\n","                att_heads = att_heads,\n","                att_mid_dim = att_mid_dim,\n","                att_mid_drop = att_mid_drop,\n","                dropout = dropout\n","            )\n","            self.layers.append(sublayer)\n","        \n","        self.proj = nn.Linear(embed_dim * (layer_num + 1), embed_dim)\n","        self.layer_norm = torch.nn.LayerNorm(cfg.MODEL.BILINEAR.DECODE_DIM)\n","        \n","    def precompute(self, key, value2):\n","        keys = []\n","        value2s = []\n","        for layer in self.layers:\n","            k, v = layer.precompute(key, value2)\n","            keys.append(k)\n","            value2s.append(v)\n","\n","        return torch.cat(keys, dim=-1), torch.cat(value2s, dim=-1)\n","\n","    def forward(self, gv_feat, att_feats, att_mask, p_att_feats=None, precompute=False):\n","        if precompute == True:\n","            dim = p_att_feats.size()[-1]\n","            keys = p_att_feats.narrow(-1, 0, dim // 2)\n","            value2s = p_att_feats.narrow(-1, dim // 2, dim // 2)\n","            dim = keys.size()[-1] // len(self.layers)\n","    \n","        if gv_feat.shape[-1] == 1:  # empty gv_feat\n","            if att_mask is not None:\n","                gv_feat = (torch.sum(att_feats * att_mask.unsqueeze(-1), 1) / torch.sum(att_mask.unsqueeze(-1), 1))\n","            else:\n","                gv_feat = torch.mean(att_feats, 1)\n","\n","        feat_arr = [gv_feat]\n","        for i, layer in enumerate(self.layers):\n","            key = keys.narrow(-1, i * dim, dim) if precompute else att_feats\n","            value2 = value2s.narrow(-1, i * dim, dim) if precompute else att_feats\n","            gv_feat = layer(gv_feat, key, att_mask, gv_feat, value2, precompute)\n","            feat_arr.append(gv_feat)\n","\n","        gv_feat = torch.cat(feat_arr, dim=-1)\n","        gv_feat = self.proj(gv_feat)\n","        gv_feat = self.layer_norm(gv_feat)\n","    \n","        return gv_feat, att_feats\n","    \n","class FeedForwardBlock(nn.Module):\n","    def __init__(self, embed_dim, ffn_embed_dim, \n","        relu_dropout, dropout):\n","        super(FeedForwardBlock, self).__init__()\n","\n","        self.fc1 = nn.Linear(embed_dim, ffn_embed_dim)\n","        self.fc2 = nn.Linear(ffn_embed_dim, embed_dim)\n","        self.dropout = dropout\n","        self.relu_dropout = relu_dropout\n","        self.layer_norms = torch.nn.LayerNorm(embed_dim)\n","\n","    def forward(self, x):\n","        residual = x\n","        x = F.relu(self.fc1(x))\n","        x = F.dropout(x, p=self.relu_dropout, training=self.training)\n","        x = self.fc2(x)\n","        x = F.dropout(x, p=self.dropout, training=self.training)\n","        x = residual + x\n","        x = self.layer_norms(x)\n","        return x\n","    \n","__factory_block = {\n","    'FeedForward': FeedForwardBlock,\n","    'LowRankBilinearEnc': LowRankBilinearEncBlock,\n","    'LowRankBilinearDec': LowRankBilinearDecBlock,\n","}\n","\n","def names_block():\n","    return sorted(__factory_block.keys())\n","\n","def create_block(name, *args, **kwargs):\n","    if name not in __factory_block:\n","        raise KeyError(\"Unknown blocks:\", name)\n","    return __factory_block[name](*args, **kwargs)\n","\n","\n","class BasicModel(nn.Module):\n","    def __init__(self):\n","        super(BasicModel, self).__init__()\n","\n","    def select(self, batch_size, beam_size, t, candidate_logprob):\n","        selected_logprob, selected_idx = torch.sort(candidate_logprob.view(batch_size, -1), -1, descending=True)\n","        selected_logprob, selected_idx = selected_logprob[:, :beam_size], selected_idx[:, :beam_size]\n","        return selected_idx, selected_logprob\n","\n","    def beam_search(self, init_state, init_logprobs, **kwargs):\n","        all_logprobs = []\n","        def add_diversity(beam_seq_table, logprobsf, t, divm, diversity_lambda, bdash):\n","            local_time = t - divm\n","            unaug_logprobsf = logprobsf.clone()\n","            for prev_choice in range(divm):\n","                prev_decisions = beam_seq_table[prev_choice][local_time]\n","                for sub_beam in range(bdash):\n","                    for prev_labels in range(bdash):\n","                        logprobsf[sub_beam][prev_decisions[prev_labels]] = logprobsf[sub_beam][prev_decisions[prev_labels]] - diversity_lambda\n","            return unaug_logprobsf\n","\n","        def beam_step(logprobsf, unaug_logprobsf, beam_size, t, beam_seq, beam_seq_logprobs, beam_logprobs_sum, state):\n","        \n","            ys,ix = torch.sort(logprobsf,1,True)\n","            candidates = []\n","            cols = min(beam_size, ys.size(1))\n","            rows = beam_size\n","            if t == 0:\n","                rows = 1\n","            for c in range(cols): # for each column (word, essentially)\n","                for q in range(rows): # for each beam expansion\n","                    #compute logprob of expanding beam q with word in (sorted) position c\n","                    local_logprob = ys[q,c].item()\n","                    candidate_logprob = beam_logprobs_sum[q] + local_logprob\n","                    local_unaug_logprob = unaug_logprobsf[q,ix[q,c]]\n","                    candidates.append({'c':ix[q,c], 'q':q, 'p':candidate_logprob, 'r':local_unaug_logprob})\n","            candidates = sorted(candidates,  key=lambda x: -x['p'])\n","            \n","            new_state = [_.clone() for _ in state]\n","            if t >= 1:\n","                beam_seq_prev = beam_seq[:t].clone()\n","                beam_seq_logprobs_prev = beam_seq_logprobs[:t].clone()\n","            for vix in range(beam_size):\n","                v = candidates[vix]\n","                if t >= 1:\n","                    beam_seq[:t, vix] = beam_seq_prev[:, v['q']]\n","                    beam_seq_logprobs[:t, vix] = beam_seq_logprobs_prev[:, v['q']]\n","                for state_ix in range(len(new_state)):\n","                    new_state[state_ix][:, vix] = state[state_ix][:, v['q']] # dimension one is time step\n","                beam_seq[t, vix] = v['c'] # c'th word is the continuation\n","                beam_seq_logprobs[t, vix] = v['r'] # the raw logprob here\n","                beam_logprobs_sum[vix] = v['p'] # the new (sum) logprob along this beam\n","            state = new_state\n","            return beam_seq,beam_seq_logprobs,beam_logprobs_sum,state,candidates\n","\n","        beam_size = kwargs['BEAM_SIZE']\n","        group_size = 1 #kwargs['GROUP_SIZE']\n","        diversity_lambda = 0.5 #kwargs['DIVERSITY_LAMBDA']\n","        constraint = False #kwargs['CONSTRAINT']\n","        max_ppl = False #kwargs['MAX_PPL']\n","        bdash = beam_size // group_size\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","        beam_seq_table = [torch.LongTensor(cfg.MODEL.SEQ_LEN, bdash).zero_() for _ in range(group_size)]\n","        beam_seq_logprobs_table = [torch.FloatTensor(cfg.MODEL.SEQ_LEN, bdash).zero_() for _ in range(group_size)]\n","        beam_logprobs_sum_table = [torch.zeros(bdash) for _ in range(group_size)]\n","\n","        done_beams_table = [[] for _ in range(group_size)]\n","        state_table = [list(torch.unbind(_)) for _ in torch.stack(init_state).chunk(group_size, 2)]\n","        logprobs_table = list(init_logprobs.chunk(group_size, 0))\n","\n","        for t in range(cfg.MODEL.SEQ_LEN + group_size - 1):\n","            for divm in range(group_size): \n","                if t >= divm and t <= cfg.MODEL.SEQ_LEN + divm - 1:\n","                    logprobsf = logprobs_table[divm].data.float()\n","                    if constraint and t-divm > 0:\n","                        logprobsf.scatter_(1, beam_seq_table[divm][t-divm-1].unsqueeze(1).to(device), float('-inf'))\n","                    logprobsf[:,logprobsf.size(1)-1] -= 1000  \n","\n","                    unaug_logprobsf = add_diversity(beam_seq_table,logprobsf,t,divm,diversity_lambda,bdash)\n","\n","                    beam_seq_table[divm],\\\n","                    beam_seq_logprobs_table[divm],\\\n","                    beam_logprobs_sum_table[divm],\\\n","                    state_table[divm],\\\n","                    candidates_divm = beam_step(logprobsf,\n","                                                unaug_logprobsf,\n","                                                bdash,\n","                                                t-divm,\n","                                                beam_seq_table[divm],\n","                                                beam_seq_logprobs_table[divm],\n","                                                beam_logprobs_sum_table[divm],\n","                                                state_table[divm])\n","\n","                    all_logprobs.append(logprobsf)\n","                    for vix in range(bdash):\n","                        if beam_seq_table[divm][t-divm,vix] == 0 or t == cfg.MODEL.SEQ_LEN + divm - 1:\n","                            final_beam = {\n","                                'seq': beam_seq_table[divm][:, vix].clone(), \n","                                'logps': beam_seq_logprobs_table[divm][:, vix].clone(),\n","                                'unaug_p': beam_seq_logprobs_table[divm][:, vix].sum().item(),\n","                                'p': beam_logprobs_sum_table[divm][vix].item()\n","                            }\n","                            if max_ppl:\n","                                final_beam['p'] = final_beam['p'] / (t-divm+1)\n","                            done_beams_table[divm].append(final_beam)\n","                            beam_logprobs_sum_table[divm][vix] = -1000\n","\n","                    wt = beam_seq_table[divm][t-divm]\n","                    kwargs[cfg.PARAM.WT] = wt.to(device) #cuda()\n","                    kwargs[cfg.PARAM.STATE] = state_table[divm]\n","                    logprobs_table[divm], state_table[divm], _ = self.get_logprobs_state(**kwargs)\n","\n","        done_beams_table = [sorted(done_beams_table[i], key=lambda x: -x['p'])[:bdash] for i in range(group_size)]\n","        done_beams = reduce(lambda a,b:a+b, done_beams_table)\n","        return done_beams, all_logprobs\n","\n","\n","class AttBasicModel(BasicModel):\n","    def __init__(self):\n","        super(AttBasicModel, self).__init__()\n","        self.ss_prob = 0.0                               # Schedule sampling probability\n","        self.vocab_size = cfg.MODEL.VOCAB_SIZE + 1      \n","        self.att_dim = cfg.MODEL.ATT_FEATS_EMBED_DIM \\\n","            if cfg.MODEL.ATT_FEATS_EMBED_DIM > 0 else cfg.MODEL.ATT_FEATS_DIM \n","\n","        # word embed\n","        sequential = [nn.Embedding(self.vocab_size, cfg.MODEL.WORD_EMBED_DIM)]\n","        sequential.append(activation(cfg.MODEL.WORD_EMBED_ACT))\n","        if cfg.MODEL.WORD_EMBED_NORM == True: \n","            sequential.append(nn.LayerNorm(cfg.MODEL.WORD_EMBED_DIM))\n","        if cfg.MODEL.DROPOUT_WORD_EMBED > 0:\n","            sequential.append(nn.Dropout(cfg.MODEL.DROPOUT_WORD_EMBED))\n","        self.word_embed = nn.Sequential(*sequential)\n","\n","        # global visual feat embed\n","        sequential = []\n","        if cfg.MODEL.GVFEAT_EMBED_DIM > 0: \n","            sequential.append(nn.Linear(cfg.MODEL.GVFEAT_DIM, cfg.MODEL.GVFEAT_EMBED_DIM))\n","        sequential.append(activation(cfg.MODEL.GVFEAT_EMBED_ACT))\n","        if cfg.MODEL.DROPOUT_GV_EMBED > 0: \n","            sequential.append(nn.Dropout(cfg.MODEL.DROPOUT_GV_EMBED))\n","        self.gv_feat_embed = nn.Sequential(*sequential) if len(sequential) > 0 else None\n","\n","        # attention feats embed\n","        sequential = []\n","        if cfg.MODEL.ATT_FEATS_EMBED_DIM > 0:\n","            sequential.append(nn.Linear(cfg.MODEL.ATT_FEATS_DIM, cfg.MODEL.ATT_FEATS_EMBED_DIM))\n","        sequential.append(activation(cfg.MODEL.ATT_FEATS_EMBED_ACT))\n","        if cfg.MODEL.DROPOUT_ATT_EMBED > 0:\n","            sequential.append(nn.Dropout(cfg.MODEL.DROPOUT_ATT_EMBED))\n","        if cfg.MODEL.ATT_FEATS_NORM == True: \n","            sequential.append(torch.nn.LayerNorm(cfg.MODEL.ATT_FEATS_EMBED_DIM))\n","        self.att_embed = nn.Sequential(*sequential) if len(sequential) > 0 else None\n","\n","        self.dropout_lm  = nn.Dropout(cfg.MODEL.DROPOUT_LM) if cfg.MODEL.DROPOUT_LM > 0 else None\n","        self.logit = nn.Linear(cfg.MODEL.RNN_SIZE, self.vocab_size)\n","        self.p_att_feats = nn.Linear(self.att_dim, cfg.MODEL.ATT_HIDDEN_SIZE) \\\n","            if cfg.MODEL.ATT_HIDDEN_SIZE > 0 else None \n","\n","        # bilinear\n","        if cfg.MODEL.BILINEAR.DIM > 0:\n","            self.p_att_feats = None\n","            self.encoder_layers = create_block(\n","                cfg.MODEL.BILINEAR.ENCODE_BLOCK,\n","                embed_dim = cfg.MODEL.BILINEAR.DIM, \n","                att_type = cfg.MODEL.BILINEAR.ATTTYPE,\n","                att_heads = cfg.MODEL.BILINEAR.HEAD,\n","                att_mid_dim = cfg.MODEL.BILINEAR.ENCODE_ATT_MID_DIM,\n","                att_mid_drop = cfg.MODEL.BILINEAR.ENCODE_ATT_MID_DROPOUT,\n","                dropout = cfg.MODEL.BILINEAR.ENCODE_DROPOUT, \n","                layer_num = cfg.MODEL.BILINEAR.ENCODE_LAYERS\n","            )\n","\n","    def init_hidden(self, batch_size):\n","        return [Variable(torch.zeros(self.num_layers, batch_size, cfg.MODEL.RNN_SIZE).cuda()),\n","                Variable(torch.zeros(self.num_layers, batch_size, cfg.MODEL.RNN_SIZE).cuda())]\n","\n","    def make_kwargs(self, wt, gv_feat, att_feats, att_mask, p_att_feats, state, **kgs):\n","        kwargs = kgs\n","        kwargs[cfg.PARAM.WT] = wt\n","        kwargs[cfg.PARAM.GLOBAL_FEAT] = gv_feat\n","        kwargs[cfg.PARAM.ATT_FEATS] = att_feats\n","        kwargs[cfg.PARAM.ATT_FEATS_MASK] = att_mask\n","        kwargs[cfg.PARAM.P_ATT_FEATS] = p_att_feats\n","        kwargs[cfg.PARAM.STATE] = state\n","        return kwargs\n","\n","    def preprocess(self, **kwargs):\n","        gv_feat = kwargs[cfg.PARAM.GLOBAL_FEAT]\n","        att_feats = kwargs[cfg.PARAM.ATT_FEATS]\n","        att_mask = kwargs[cfg.PARAM.ATT_FEATS_MASK]\n","\n","        # embed gv_feat\n","        if self.gv_feat_embed is not None:\n","            gv_feat = self.gv_feat_embed(gv_feat)\n","        \n","        # embed att_feats\n","        if self.att_embed is not None:    \n","            att_feats = self.att_embed(att_feats)\n","\n","        p_att_feats = self.p_att_feats(att_feats) if self.p_att_feats is not None else None\n","\n","        # bilinear\n","        if cfg.MODEL.BILINEAR.DIM > 0:\n","            gv_feat, att_feats = self.encoder_layers(gv_feat, att_feats, att_mask)\n","            keys, value2s = self.attention.precompute(att_feats, att_feats)\n","            p_att_feats = torch.cat([keys, value2s], dim=-1)\n","\n","        return gv_feat, att_feats, att_mask, p_att_feats\n","\n","    def forward(self, **kwargs): \n","        seq = kwargs[cfg.PARAM.INPUT_SENT]\n","        gv_feat, att_feats, att_mask, p_att_feats = self.preprocess(**kwargs)\n","        gv_feat = expand_tensor(gv_feat, cfg.DATA_LOADER.SEQ_PER_IMG)\n","        att_feats = expand_tensor(att_feats, cfg.DATA_LOADER.SEQ_PER_IMG)\n","        att_mask = expand_tensor(att_mask, cfg.DATA_LOADER.SEQ_PER_IMG)\n","        p_att_feats = expand_tensor(p_att_feats, cfg.DATA_LOADER.SEQ_PER_IMG)\n","\n","        batch_size = gv_feat.size(0)\n","        state = self.init_hidden(batch_size)\n","\n","        outputs = Variable(torch.zeros(batch_size, seq.size(1), self.vocab_size).cuda())\n","        for t in range(seq.size(1)):\n","            if self.training and t >=1 and self.ss_prob > 0:\n","                prob = torch.empty(batch_size).cuda().uniform_(0, 1)\n","                mask = prob < self.ss_prob\n","                if mask.sum() == 0:\n","                    wt = seq[:,t].clone()\n","                else:\n","                    ind = mask.nonzero().view(-1)\n","                    wt = seq[:, t].data.clone()\n","                    prob_prev = torch.exp(outputs[:, t-1].detach())\n","                    wt = wt.long()\n","                    wt.index_copy_(0, ind, torch.multinomial(prob_prev, 1).view(-1).index_select(0, ind))\n","            else:\n","                wt = seq[:,t].clone()\n","\n","            if t >= 1 and seq[:, t].max() == 0:\n","                break\n","            \n","            kwargs = self.make_kwargs(wt, gv_feat, att_feats, att_mask, p_att_feats, state)\n","            output, state = self.Forward(**kwargs)\n","            if self.dropout_lm is not None:\n","                output = self.dropout_lm(output)\n","\n","            logit = self.logit(output)\n","            outputs[:, t] = logit\n","\n","        return outputs\n","\n","    def get_logprobs_state(self, **kwargs):\n","        output, state = self.Forward(**kwargs)\n","        logprobs = F.log_softmax(self.logit(output), dim=1)\n","        outputs = self.logit(output)\n","        return logprobs, state, outputs\n","    \n","    def _expand_state(self, batch_size, beam_size, cur_beam_size, state, selected_beam):\n","        shape = [int(sh) for sh in state.shape]\n","        beam = selected_beam\n","        for _ in shape[2:]:\n","            beam = beam.unsqueeze(-1)\n","        beam = beam.unsqueeze(0)\n","        beam_long = beam.long()\n","        state = torch.gather(\n","            state.view(*([shape[0], batch_size, cur_beam_size] + shape[2:])), 2,\n","            beam_long.expand(*([shape[0], batch_size, beam_size] + shape[2:]))\n","        )\n","        state = state.view(*([shape[0], -1, ] + shape[2:]))\n","        return state\n","\n","    def decode_beam(self, **kwargs):\n","        beam_size = kwargs['BEAM_SIZE']\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        gv_feat, att_feats, att_mask, p_att_feats = self.preprocess(**kwargs)\n","        batch_size = gv_feat.size(0)\n","    \n","        sents = Variable(torch.zeros((cfg.MODEL.SEQ_LEN, batch_size), dtype=torch.long).to(device))#cuda())\n","        logprobs = Variable(torch.zeros(cfg.MODEL.SEQ_LEN, batch_size).to(device))#cuda())   \n","        self.done_beams = [[] for _ in range(batch_size)]\n","        all_logprobs_batch = []\n","        for n in range(batch_size):\n","            state = self.init_hidden(beam_size)\n","            gv_feat_beam = gv_feat[n:n+1].expand(beam_size, gv_feat.size(1)).contiguous()\n","            att_feats_beam = att_feats[n:n+1].expand(*((beam_size,)+att_feats.size()[1:])).contiguous()\n","            att_mask_beam = att_mask[n:n+1].expand(*((beam_size,)+att_mask.size()[1:]))\n","            p_att_feats_beam = p_att_feats[n:n+1].expand(*((beam_size,)+p_att_feats.size()[1:])).contiguous() if p_att_feats is not None else None\n","    \n","            wt = Variable(torch.ones(beam_size, dtype=torch.long).to(device)) * 3\n","            kwargs = self.make_kwargs(wt, gv_feat_beam, att_feats_beam, att_mask_beam, p_att_feats_beam, state, **kwargs)\n","            logprobs_t, state, _ = self.get_logprobs_state(**kwargs)\n","    \n","            self.done_beams[n], all_logprobs = self.beam_search(state, logprobs_t, **kwargs)\n","            all_logprobs = torch.stack(all_logprobs)\n","            all_logprobs_batch.append(all_logprobs)\n","            all_logprobs_fix = torch.cat(all_logprobs_batch, dim=0)\n","            sents[:, n] = self.done_beams[n][0]['seq'] \n","            logprobs[:, n] = self.done_beams[n][0]['logps']\n","\n","        return sents.transpose(0, 1), logprobs.transpose(0, 1), all_logprobs_fix\n","\n","    def decode(self, **kwargs):\n","        greedy_decode = kwargs['GREEDY_DECODE']\n"," \n","        gv_feat, att_feats, att_mask, p_att_feats = self.preprocess(**kwargs)\n","        batch_size = gv_feat.size(0)\n","        state = self.init_hidden(batch_size)\n","\n","        sents = Variable(torch.zeros((batch_size, cfg.MODEL.SEQ_LEN), dtype=torch.long).cuda())\n","        logprobs = Variable(torch.zeros(batch_size, cfg.MODEL.SEQ_LEN).cuda())\n","        outputs = Variable(torch.zeros(batch_size, cfg.MODEL.SEQ_LEN, self.vocab_size).cuda())   \n","        wt = Variable(torch.ones(batch_size, dtype=torch.long).cuda()) * 3\n","        unfinished = wt.eq(wt)\n","        all_logprobs = []\n","        \n","        for t in range(cfg.MODEL.SEQ_LEN):\n","            kwargs = self.make_kwargs(wt, gv_feat, att_feats, att_mask, p_att_feats, state)\n","            logprobs_t, state, output = self.get_logprobs_state(**kwargs)\n","            all_logprobs.append(logprobs_t)\n","            \n","            if greedy_decode:\n","                logP_t, wt = torch.max(logprobs_t, 1)\n","            else:\n","                probs_t = torch.exp(logprobs_t)\n","                wt = torch.multinomial(probs_t, 1)\n","                logP_t = logprobs_t.gather(1, wt)\n","\n","            wt = wt.view(-1).long()\n","            unfinished = unfinished * ((wt != 0) & (wt != 4))\n","            wt = wt * unfinished.type_as(wt)\n","            sents[:,t] = wt\n","            logprobs[:,t] = logP_t.view(-1)\n","            outputs[:,t] = output\n","\n","            if unfinished.sum() == 0:\n","                break\n","  \n","        return sents, logprobs, all_logprobs, outputs\n","\n","\n","class XLAN(AttBasicModel):\n","    def __init__(self):\n","        super(XLAN, self).__init__()\n","        self.num_layers = 2\n","\n","        rnn_input_size = cfg.MODEL.RNN_SIZE + cfg.MODEL.BILINEAR.DECODE_DIM\n","        self.att_lstm = nn.LSTMCell(rnn_input_size, cfg.MODEL.RNN_SIZE)\n","        self.ctx_drop = nn.Dropout(cfg.MODEL.DROPOUT_LM)\n","\n","        self.attention = create_block(            \n","            cfg.MODEL.BILINEAR.DECODE_BLOCK, \n","            embed_dim = cfg.MODEL.BILINEAR.DECODE_DIM, \n","            att_type = cfg.MODEL.BILINEAR.ATTTYPE,\n","            att_heads = cfg.MODEL.BILINEAR.DECODE_HEAD,\n","            att_mid_dim = cfg.MODEL.BILINEAR.DECODE_ATT_MID_DIM,\n","            att_mid_drop = cfg.MODEL.BILINEAR.DECODE_ATT_MID_DROPOUT,\n","            dropout = cfg.MODEL.BILINEAR.DECODE_DROPOUT, \n","            layer_num = cfg.MODEL.BILINEAR.DECODE_LAYERS\n","        )\n","        self.att2ctx = nn.Sequential(\n","            nn.Linear(cfg.MODEL.BILINEAR.DECODE_DIM + cfg.MODEL.RNN_SIZE, 2 * cfg.MODEL.RNN_SIZE), \n","            nn.GLU()\n","        )\n","\n","    def Forward(self, **kwargs):\n","\n","        wt = kwargs[cfg.PARAM.WT]\n","        att_feats = kwargs[cfg.PARAM.ATT_FEATS]\n","        att_mask = kwargs[cfg.PARAM.ATT_FEATS_MASK]\n","        state = kwargs[cfg.PARAM.STATE]\n","        gv_feat = kwargs[cfg.PARAM.GLOBAL_FEAT]\n","        p_att_feats = kwargs[cfg.PARAM.P_ATT_FEATS]\n","        \n","        if gv_feat.shape[-1] == 1:  # empty gv_feat\n","            if att_mask is not None:\n","                gv_feat = torch.sum(att_feats * att_mask.unsqueeze(-1), 1) / torch.sum(att_mask.unsqueeze(-1), 1)\n","            else:\n","                gv_feat = torch.mean(att_feats, 1)\n","\n","        xt = self.word_embed(wt)\n","        \n","        h_att, c_att = self.att_lstm(torch.cat([xt, gv_feat + self.ctx_drop(state[0][1])], 1), (state[0][0], state[1][0]))\n","\n","        att, _ = self.attention(h_att, att_feats, att_mask, p_att_feats, precompute=True)\n","        ctx_input = torch.cat([att, h_att], 1)\n","\n","        output = self.att2ctx(ctx_input)\n","        state = [torch.stack((h_att, output)), torch.stack((c_att, state[1][1]))]\n","        \n","        return output, state\n","    \n","__factory_model = {\n","    'XLAN': XLAN,\n","}\n","\n","def names_model():\n","    return sorted(__factory_model.keys())\n","\n","def create_model(name, *args, **kwargs):\n","    if name not in __factory_model:\n","        raise KeyError(\"Unknown caption model:\", name)\n","    return __factory_model[name](*args, **kwargs)\n","\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:41.924181Z","iopub.status.busy":"2024-07-10T14:41:41.923772Z","iopub.status.idle":"2024-07-10T14:41:41.942712Z","shell.execute_reply":"2024-07-10T14:41:41.941747Z","shell.execute_reply.started":"2024-07-10T14:41:41.924152Z"},"trusted":true},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import torch\n","import torch.utils.data as data\n","import pickle\n","\n","class CocoDataset(data.Dataset):\n","    def __init__(\n","        self, \n","        image_ids_path, \n","        input_seq, \n","        target_seq,\n","        gv_feat_path, \n","        att_feats_folder, \n","        seq_per_img,\n","        max_feat_num\n","    ):\n","        self.max_feat_num = max_feat_num\n","        self.seq_per_img = seq_per_img\n","        self.image_ids = load_lines(image_ids_path)\n","        self.att_feats_folder = att_feats_folder if len(att_feats_folder) > 0 else None\n","        self.gv_feat = pickle.load(open(gv_feat_path, 'rb'), encoding='bytes') if len(gv_feat_path) > 0 else None\n","\n","        if input_seq is not None:\n","            self.input_seq = pickle.load(open(input_seq, 'rb'), encoding='bytes')\n","            self.target_seq = pickle.load(open(target_seq, 'rb'), encoding='bytes')\n","            self.seq_len = len(self.input_seq[self.image_ids[0]][0,:])\n","        elif target_seq is not None:\n","            self.target_seq = pickle.load(open(target_seq, 'rb'), encoding='bytes')\n","            self.input_seq = None\n","            self.seq_len = -1\n","        else:\n","            self.seq_len = -1\n","            self.input_seq = None\n","            self.target_seq = None\n","         \n","    def set_seq_per_img(self, seq_per_img):\n","        self.seq_per_img = seq_per_img\n","\n","    def __len__(self):\n","        return len(self.image_ids)\n","\n","    def __getitem__(self, index):\n","        image_id = self.image_ids[index]\n","        indices = np.array([index]).astype('int')\n","\n","        if self.gv_feat is not None:\n","            gv_feat = self.gv_feat[image_id]\n","            gv_feat = np.array(gv_feat).astype('float32')\n","        else:\n","            gv_feat = np.zeros((1,1))\n","\n","        if self.att_feats_folder is not None:\n","            att_feats = np.load(os.path.join(self.att_feats_folder, str(image_id) + '.npz'))['feat']\n","            att_feats = np.array(att_feats).astype('float32')\n","        else:\n","            att_feats = np.zeros((1,1))\n","        \n","        if self.max_feat_num > 0 and att_feats.shape[0] > self.max_feat_num:\n","           att_feats = att_feats[:self.max_feat_num, :]\n","\n","        if self.seq_len < 0:\n","            if self.target_seq is not None:\n","                target_seq = self.target_seq[image_id][0, :].reshape(1, -1)\n","                return indices, target_seq, gv_feat, att_feats\n","            else:\n","                return indices, gv_feat, att_feats\n","\n","        input_seq = np.zeros((self.seq_per_img, self.seq_len), dtype='int')\n","        target_seq = np.zeros((self.seq_per_img, self.seq_len), dtype='int')\n","           \n","        n = len(self.input_seq[image_id])   \n","        if n >= self.seq_per_img:\n","            sid = 0\n","            ixs = random.sample(range(n), self.seq_per_img)                \n","        else:\n","            sid = n\n","            ixs = random.sample(range(n), self.seq_per_img - n)\n","            input_seq[0:n, :] = self.input_seq[image_id]\n","            target_seq[0:n, :] = self.target_seq[image_id]\n","           \n","        for i, ix in enumerate(ixs):\n","            input_seq[sid + i] = self.input_seq[image_id][ix,:]\n","            target_seq[sid + i] = self.target_seq[image_id][ix,:]\n","        return indices, input_seq, target_seq, gv_feat, att_feats\n","    \n","class CocoVal(data.Dataset):\n","    def __init__(\n","        self, \n","        image_ids_path, \n","        input_seq, \n","        target_seq,\n","        gv_feat_path, \n","        att_feats_folder, \n","        seq_per_img,\n","        max_feat_num\n","    ):\n","        self.max_feat_num = max_feat_num\n","        self.seq_per_img = seq_per_img\n","        self.image_ids = load_lines(image_ids_path)\n","        self.att_feats_folder = att_feats_folder if len(att_feats_folder) > 0 else None\n","        self.gv_feat = pickle.load(open(gv_feat_path, 'rb'), encoding='bytes') if len(gv_feat_path) > 0 else None\n","\n","        if input_seq is not None:\n","            self.input_seq = pickle.load(open(input_seq, 'rb'), encoding='bytes')\n","            self.target_seq = pickle.load(open(target_seq, 'rb'), encoding='bytes')\n","            self.seq_len = len(self.input_seq[self.image_ids[0]][0,:])\n","        elif target_seq is not None:\n","            self.target_seq = pickle.load(open(target_seq, 'rb'), encoding='bytes')\n","            self.input_seq = None\n","            self.seq_len = -1\n","        else:\n","            self.seq_len = -1\n","            self.input_seq = None\n","            self.target_seq = None\n","         \n","    def set_seq_per_img(self, seq_per_img):\n","        self.seq_per_img = seq_per_img\n","\n","    def __len__(self):\n","        return len(self.image_ids)\n","\n","    def __getitem__(self, index):\n","        image_id = self.image_ids[index]\n","        indices = np.array([index]).astype('int')\n","\n","        if self.gv_feat is not None:\n","            gv_feat = self.gv_feat[image_id]\n","            gv_feat = np.array(gv_feat).astype('float32')\n","        else:\n","            gv_feat = np.zeros((1,1))\n","\n","        if self.att_feats_folder is not None:\n","            att_feats = np.load(os.path.join(self.att_feats_folder, str(image_id) + '.npz'))['feat']\n","            att_feats = np.array(att_feats).astype('float32')\n","        else:\n","            att_feats = np.zeros((1,1))\n","        \n","        if self.max_feat_num > 0 and att_feats.shape[0] > self.max_feat_num:\n","           att_feats = att_feats[:self.max_feat_num, :]\n","\n","        if self.seq_len < 0:\n","            if self.target_seq is not None:\n","                target_seq = self.target_seq[image_id][0, :].reshape(1, -1)\n","                return indices, target_seq, gv_feat, att_feats\n","            else:\n","                return indices, gv_feat, att_feats\n","\n","        input_seq = np.zeros((self.seq_per_img, self.seq_len), dtype='int')\n","        target_seq = np.zeros((self.seq_per_img, self.seq_len), dtype='int')\n","           \n","        n = len(self.input_seq[image_id])   \n","        if n >= self.seq_per_img:\n","            sid = 0\n","            ixs = random.sample(range(n), self.seq_per_img)                \n","        else:\n","            sid = n\n","            ixs = random.sample(range(n), self.seq_per_img - n)\n","            input_seq[0:n, :] = self.input_seq[image_id]\n","            target_seq[0:n, :] = self.target_seq[image_id]\n","           \n","        for i, ix in enumerate(ixs):\n","            input_seq[sid + i] = self.input_seq[image_id][ix,:]\n","            target_seq[sid + i] = self.target_seq[image_id][ix,:]\n","        return indices, target_seq, gv_feat, att_feats"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-07-11T09:58:47.633200Z","iopub.status.busy":"2024-07-11T09:58:47.632824Z","iopub.status.idle":"2024-07-11T09:58:47.663615Z","shell.execute_reply":"2024-07-11T09:58:47.662585Z","shell.execute_reply.started":"2024-07-11T09:58:47.633169Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","def sample_collate(batch):\n","    batch = [(np.array(idx), np.array(inp), np.array(tgt), np.array(gv), np.array(att)) for idx, inp, tgt, gv, att in batch]\n","    indices, input_seq, target_seq, gv_feat, att_feats = zip(*batch)\n","    \n","    indices = np.stack(indices, axis=0).reshape(-1)\n","    input_seq = torch.cat([torch.from_numpy(b) for b in input_seq], 0)\n","    target_seq = torch.cat([torch.from_numpy(b) for b in target_seq], 0)\n","    gv_feat = torch.cat([torch.from_numpy(b) for b in gv_feat], 0)\n","    \n","    max_att_num = 1\n","    \n","    feat_arr = []\n","    mask_arr = []\n","    \n","    for i in range(len(att_feats)):\n","        tmp_feat = np.zeros((1, max_att_num, 2048), dtype=np.float32)\n","        tmp_feat[0, 0, :] = att_feats[i]\n","        feat_arr.append(torch.from_numpy(tmp_feat))\n","        \n","        tmp_mask = np.zeros((1, max_att_num), dtype=np.float32)\n","        tmp_mask[0, 0] = 1\n","        mask_arr.append(torch.from_numpy(tmp_mask))\n","    \n","    att_feats = torch.cat(feat_arr, 0)\n","    att_mask = torch.cat(mask_arr, 0)\n","    \n","    return indices, input_seq, target_seq, gv_feat, att_feats, att_mask\n","\n","def sample_collate_val(batch):\n","    batch = [(np.array(idx), np.array(tgt), np.array(gv), np.array(att)) for idx, tgt, gv, att in batch]\n","    indices, valtarget_seq, gv_feat, att_feats = zip(*batch)\n","    \n","    indices = np.stack(indices, axis=0).reshape(-1)\n","    gv_feat = torch.cat([torch.from_numpy(b) for b in gv_feat], 0)\n","    valtarget_seq = torch.cat([torch.from_numpy(b) for b in valtarget_seq], 0)\n","\n","    max_att_num = 1\n","\n","    feat_arr = []\n","    mask_arr = []\n","    \n","    for i in range(len(att_feats)):\n","        tmp_feat = np.zeros((1, max_att_num, 2048), dtype=np.float32)\n","        tmp_feat[0, 0, :] = att_feats[i]\n","        feat_arr.append(torch.from_numpy(tmp_feat))\n","        \n","        tmp_mask = np.zeros((1, max_att_num), dtype=np.float32)\n","        tmp_mask[0, 0] = 1\n","        mask_arr.append(torch.from_numpy(tmp_mask))\n","\n","    att_feats = torch.cat(feat_arr, 0)\n","    att_mask = torch.cat(mask_arr, 0)\n","\n","    return indices, valtarget_seq, gv_feat, att_feats, att_mask\n","\n","def sample_collate_test(batch):\n","    batch = [(np.array(idx), np.array(gv), np.array(att)) for idx, gv, att in batch]\n","    indices, gv_feat, att_feats = zip(*batch)\n","    \n","    indices = np.stack(indices, axis=0).reshape(-1)\n","    gv_feat = torch.cat([torch.from_numpy(b) for b in gv_feat], 0)\n","\n","    max_att_num = 1\n","\n","    feat_arr = []\n","    mask_arr = []\n","    \n","    for i in range(len(att_feats)):\n","        tmp_feat = np.zeros((1, max_att_num, 2048), dtype=np.float32)\n","        tmp_feat[0, 0, :] = att_feats[i]\n","        feat_arr.append(torch.from_numpy(tmp_feat))\n","  \n","        tmp_mask = np.zeros((1, max_att_num), dtype=np.float32)\n","        tmp_mask[0, 0] = 1\n","        mask_arr.append(torch.from_numpy(tmp_mask))\n","\n","    att_feats = torch.cat(feat_arr, 0)\n","    att_mask = torch.cat(mask_arr, 0)\n","\n","    return indices, gv_feat, att_feats, att_mask\n","\n","def load_train(coco_set):\n","    loader = torch.utils.data.DataLoader(\n","        coco_set, \n","        batch_size = cfg.TRAIN.BATCH_SIZE,\n","        shuffle = True, \n","        num_workers = 0, #cfg.DATA_LOADER.NUM_WORKERS, \n","        drop_last = cfg.DATA_LOADER.DROP_LAST, \n","        pin_memory = cfg.DATA_LOADER.PIN_MEMORY,\n","        sampler = None, \n","        collate_fn = sample_collate\n","    )\n","    return loader\n","\n","def load_test(image_ids_path, gv_feat_path, att_feats_folder):\n","    coco_set = CocoDataset(\n","        image_ids_path = image_ids_path, \n","        input_seq = None, \n","        target_seq = None, \n","        gv_feat_path = gv_feat_path, \n","        att_feats_folder = att_feats_folder,\n","        seq_per_img = 1, \n","        max_feat_num = cfg.DATA_LOADER.MAX_FEAT\n","    )\n","    \n","    loader = torch.utils.data.DataLoader(\n","        coco_set, \n","        batch_size = 1,\n","        shuffle = False, \n","        num_workers = 0, #cfg.DATA_LOADER.NUM_WORKERS, \n","        drop_last = False, \n","        pin_memory = cfg.DATA_LOADER.PIN_MEMORY, \n","        collate_fn = sample_collate_test\n","    )\n","    return loader\n","\n","def load_val(image_ids_path, valtarget_seq, gv_feat_path, att_feats_folder):\n","    coco_set = CocoVal(\n","        image_ids_path = image_ids_path, \n","        input_seq = None, \n","        target_seq = valtarget_seq, \n","        gv_feat_path = gv_feat_path, \n","        att_feats_folder = att_feats_folder,\n","        seq_per_img = 1, \n","        max_feat_num = cfg.DATA_LOADER.MAX_FEAT\n","    )\n","    coco_set = CocoDataset(\n","        image_ids_path = image_ids_path, \n","        input_seq = None, \n","        target_seq = valtarget_seq, \n","        gv_feat_path = gv_feat_path, \n","        att_feats_folder = att_feats_folder,\n","        seq_per_img = 1, \n","        max_feat_num = cfg.DATA_LOADER.MAX_FEAT\n","    )\n","    \n","    loader = torch.utils.data.DataLoader(\n","        coco_set, \n","        batch_size = 1,\n","        shuffle = False, \n","        num_workers = 0, \n","        drop_last = False, \n","        pin_memory = cfg.DATA_LOADER.PIN_MEMORY, \n","        collate_fn = sample_collate_val\n","    )\n","    return loader\n","\n","def setup_loader(coco_set):\n","    training_loader = load_train(coco_set)\n","    return training_loader\n","        \n","def setup_dataset():\n","    coco_set = CocoDataset(            \n","            image_ids_path = cfg.DATA_LOADER.VALT_ID, \n","            input_seq = cfg.DATA_LOADER.INPUT_SEQ_PATH, \n","            target_seq = cfg.DATA_LOADER.TARGET_SEQ_PATH,\n","            gv_feat_path = cfg.DATA_LOADER.TRAIN_GV_FEAT, \n","            att_feats_folder = cfg.DATA_LOADER.TRAIN_ATT_FEATS, \n","            seq_per_img = cfg.DATA_LOADER.SEQ_PER_IMG,\n","            max_feat_num = cfg.DATA_LOADER.MAX_FEAT\n","        )\n","    return coco_set\n","\n","class CrossEntropy(nn.Module):\n","    def __init__(self):\n","        super(CrossEntropy, self).__init__()\n","        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n","\n","    def forward(self, logit, target_seq):\n","        logit = logit.view(-1, logit.shape[-1])\n","        target_seq = target_seq.view(-1)\n","        loss = self.criterion(logit, target_seq)\n","        return loss, {'CrossEntropy Loss': loss.item()}\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:46.615061Z","iopub.status.busy":"2024-07-10T14:41:46.614241Z","iopub.status.idle":"2024-07-10T14:41:46.626783Z","shell.execute_reply":"2024-07-10T14:41:46.625827Z","shell.execute_reply.started":"2024-07-10T14:41:46.615028Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import numpy as np\n","import torch\n","import tqdm\n","import json\n","\n","class EvalTest(object):\n","    def __init__(\n","        self,\n","        eval_ids,\n","        gv_feat,\n","        att_feats\n","    ):\n","        super(EvalTest, self).__init__()\n","        self.vocab = load_vocab(cfg.INFERENCE.VOCAB)\n","\n","        self.eval_ids = np.array(load_ids(eval_ids))\n","        self.eval_loader = load_test(eval_ids, gv_feat, att_feats)\n","\n","    def make_kwargs(self, indices, ids, gv_feat, att_feats, att_mask):\n","        kwargs = {}\n","        kwargs[cfg.PARAM.INDICES] = indices\n","        kwargs[cfg.PARAM.GLOBAL_FEAT] = gv_feat\n","        kwargs[cfg.PARAM.ATT_FEATS] = att_feats\n","        kwargs[cfg.PARAM.ATT_FEATS_MASK] = att_mask\n","        kwargs['BEAM_SIZE'] = cfg.INFERENCE.BEAM_SIZE\n","        kwargs['GREEDY_DECODE'] = cfg.INFERENCE.GREEDY_DECODE\n","        return kwargs\n","        \n","    def __call__(self, model, device):\n","        model.eval()\n","\n","        results = []\n","        with torch.no_grad():\n","            for batch_idx, (indices, gv_feat, att_feats, att_mask) in tqdm.tqdm(enumerate(self.eval_loader)):\n","                ids = self.eval_ids[indices]\n","                gv_feat = gv_feat.to(device)\n","                att_feats = att_feats.to(device)\n","                att_mask = att_mask.to(device)\n","\n","                kwargs = self.make_kwargs(indices, ids, gv_feat, att_feats, att_mask)\n","                if kwargs['BEAM_SIZE'] > 1:\n","                    seq, logprob, all_logprobs = model.module.decode_beam(**kwargs)\n","                else:\n","                    seq, logprob, all_logprobs, output = model.module.decode(**kwargs)\n","                    \n","                sents = decode_sequence(self.vocab, seq.data)\n","    \n","                for sid, sent in enumerate(sents):\n","                    result = {cfg.INFERENCE.ID_KEY: ids[sid], cfg.INFERENCE.CAP_KEY: sent}\n","                    results.append(result)\n","        \n","        model.train()\n","        return results"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:48.984334Z","iopub.status.busy":"2024-07-10T14:41:48.983721Z","iopub.status.idle":"2024-07-10T14:41:48.999467Z","shell.execute_reply":"2024-07-10T14:41:48.998594Z","shell.execute_reply.started":"2024-07-10T14:41:48.984303Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import numpy as np\n","import torch\n","import tqdm\n","import json\n","\n","class Evaler(object):\n","    def __init__(\n","        self,\n","        eval_ids,\n","        valtarget_seq,\n","        gv_feat,\n","        att_feats\n","    ):\n","        super(Evaler, self).__init__()\n","        self.vocab = load_vocab(cfg.INFERENCE.VOCAB)\n","\n","        self.eval_ids = np.array(load_ids(eval_ids))\n","        self.eval_loader = load_val(eval_ids, valtarget_seq, gv_feat, att_feats)\n","        \n","\n","    def make_kwargs(self, indices, ids, valtarget_seq, gv_feat, att_feats, att_mask):\n","        kwargs = {}\n","        kwargs[cfg.PARAM.INDICES] = indices\n","        kwargs[cfg.PARAM.TARGET_SENT] = valtarget_seq\n","        kwargs[cfg.PARAM.GLOBAL_FEAT] = gv_feat\n","        kwargs[cfg.PARAM.ATT_FEATS] = att_feats\n","        kwargs[cfg.PARAM.ATT_FEATS_MASK] = att_mask\n","        kwargs['BEAM_SIZE'] = 1 #cfg.INFERENCE.BEAM_SIZE\n","        kwargs['GREEDY_DECODE'] = cfg.INFERENCE.GREEDY_DECODE\n","        return kwargs\n","        \n","    def __call__(self, model, device):\n","        model.eval()\n","        total_loss = 0.0\n","        num_batches = 0\n","\n","        results = []\n","        with torch.no_grad():\n","            for batch_idx, (indices, valtarget_seq, gv_feat, att_feats, att_mask) in tqdm.tqdm(enumerate(self.eval_loader)):\n","                ids = self.eval_ids[indices]\n","                gv_feat = gv_feat.to(device)\n","                att_feats = att_feats.to(device)\n","                att_mask = att_mask.to(device)\n","                valtarget_seq = valtarget_seq.to(device)\n","\n","                kwargs = self.make_kwargs(indices, ids, valtarget_seq, gv_feat, att_feats, att_mask)\n","                if kwargs['BEAM_SIZE'] > 1:\n","                    seq, logprob, all_logprobs = model.module.decode_beam(**kwargs)\n","                else:\n","                    seq, logprob, all_logprobs, outputs = model.module.decode(**kwargs)\n","\n","                target_seq = kwargs[cfg.PARAM.TARGET_SENT]\n","\n","                if target_seq.dtype != torch.long:\n","                    target_seq = target_seq.long()\n","    \n","                loss, loss_info = CrossEntropy()(outputs, target_seq)\n","\n","                total_loss += loss.item()\n","                num_batches += 1\n","\n","        avg_loss = total_loss / num_batches if num_batches > 0 else 0.\n","        model.train()\n","        return avg_loss"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-07-10T14:41:51.279862Z","iopub.status.busy":"2024-07-10T14:41:51.279483Z","iopub.status.idle":"2024-07-10T14:41:51.290834Z","shell.execute_reply":"2024-07-10T14:41:51.290018Z","shell.execute_reply.started":"2024-07-10T14:41:51.279820Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import time\n","\n","\n","def display(iteration, data_time, batch_time, losses, loss_info):\n","    info_str = ' (DataTime/BatchTime: {:.3f}/{:.3f}) losses = {:.5f}'.format(data_time.avg, batch_time.avg, losses.avg)\n","    print('Iteration ' + str(iteration) + info_str + ', lr = ' + str(optim.get_lr()))\n","    \n","    for name in sorted(loss_info):\n","        print('  ' + name + ' = ' + str(loss_info[name]))\n","    \n","    data_time.reset()\n","    batch_time.reset()\n","    losses.reset()\n","\n","def make_kwargs(indices, input_seq, target_seq, gv_feat, att_feats, att_mask):\n","    seq_mask = (input_seq > 0).type(torch.cuda.LongTensor)\n","    seq_mask[:,0] += 1\n","    seq_mask_sum = seq_mask.sum(-1)\n","    max_len = int(seq_mask_sum.max())\n","    input_seq = input_seq[:, 0:max_len].contiguous()\n","    target_seq = target_seq[:, 0:max_len].contiguous()\n","\n","    kwargs = {\n","        cfg.PARAM.INDICES: indices,\n","        cfg.PARAM.INPUT_SENT: input_seq,\n","        cfg.PARAM.TARGET_SENT: target_seq,\n","        cfg.PARAM.GLOBAL_FEAT: gv_feat,\n","        cfg.PARAM.ATT_FEATS: att_feats,\n","        cfg.PARAM.ATT_FEATS_MASK: att_mask\n","    }\n","    return kwargs\n","\n","def forward(model, kwargs):\n","    logit = model(**kwargs)\n","    \n","    target_seq = kwargs[cfg.PARAM.TARGET_SENT]\n","    \n","    if target_seq.dtype != torch.long:\n","        target_seq = target_seq.long()\n","    \n","    loss, loss_info = CrossEntropy()(logit, target_seq)\n","    return loss, loss_info"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-11T10:22:20.161422Z","iopub.status.busy":"2024-07-11T10:22:20.160567Z","iopub.status.idle":"2024-07-11T10:25:49.683931Z","shell.execute_reply":"2024-07-11T10:25:49.682362Z","shell.execute_reply.started":"2024-07-11T10:22:20.161388Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","class Trainer:\n","    def __init__(self, model, optimizer, dataloader, device, save_path):\n","        random.seed(cfg.SEED)\n","        torch.manual_seed(cfg.SEED)\n","        torch.cuda.manual_seed_all(cfg.SEED)\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.dataloader = dataloader\n","        self.device = device\n","        self.save_path = save_path\n","        self.train_losses = []\n","        self.valid_losses = []\n","        self.data_time = AverageMeter()\n","        self.batch_time = AverageMeter()\n","        self.losses = AverageMeter()\n","\n","    def evaluate(self, model):\n","        self.model = model\n","        self.model.eval()\n","        self.evaler = Evaler(\n","            eval_ids = cfg.DATA_LOADER.VALT_ID,\n","            valtarget_seq = cfg.DATA_LOADER.TARGET_SEQ_PATH,\n","            gv_feat = cfg.DATA_LOADER.TRAIN_GV_FEAT,\n","            att_feats = cfg.DATA_LOADER.TRAIN_ATT_FEATS)\n","        avg_loss = self.evaler(self.model, self.device)\n","    \n","        return avg_loss\n","    \n","    def save_model(self, epoch):\n","        save_file_path = os.path.join(self.save_path, f'model4_epoch_{epoch + 1}.pth')\n","        torch.save(self.model.state_dict(), save_file_path)\n","\n","    def scheduled_sampling(self, epoch):\n","        if epoch > cfg.TRAIN.SCHEDULED_SAMPLING.START:\n","            frac = (epoch - cfg.TRAIN.SCHEDULED_SAMPLING.START) // cfg.TRAIN.SCHEDULED_SAMPLING.INC_EVERY\n","            ss_prob = min(cfg.TRAIN.SCHEDULED_SAMPLING.INC_PROB * frac, cfg.TRAIN.SCHEDULED_SAMPLING.MAX_PROB)\n","            self.model.module.ss_prob = ss_prob\n","\n","    def plot_losses(self):\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(range(1, len(self.train_losses) + 1), self.train_losses, 'b-', label='Training Loss')\n","        plt.plot(range(1, len(self.valid_losses) + 1), self.valid_losses, 'r-', label='Validation Loss')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        plt.title('Model 4 Training and Validation Loss')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()\n","\n","    def train(self, num_epoch):\n","        self.model = torch.nn.DataParallel(self.model).to(device)\n","        self.model.train()\n","        optimizer = self.optimizer\n","        optimizer.zero_grad()\n","        scheduler1 = optimizer.scheduler1()\n","        scheduler2 = optimizer.scheduler2()\n","        scheduler3 = optimizer.scheduler3()\n","\n","        total_batches = len(self.dataloader)\n","\n","        for epoch in range(num_epoch):\n","            print(f\"Epoch {epoch + 1}/{num_epoch}\")\n","            training_loader = self.dataloader\n","\n","            training_loss = 0.0\n","            epoch_start_time = time.time()\n","            \n","            for batch_idx, (indices, input_seq, target_seq, gv_feat, att_feats, att_mask) in enumerate(training_loader):\n","                start_time = time.time() \n","\n","                input_seq = input_seq.to(device)\n","                target_seq = target_seq.to(device)\n","                gv_feat = gv_feat.to(device)\n","                att_feats = att_feats.to(device)\n","                att_mask = att_mask.to(device)\n","\n","                kwargs = make_kwargs(indices, input_seq, target_seq, gv_feat, att_feats, att_mask)\n","                loss, loss_info = forward(self.model, kwargs)\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                training_loss += loss.item()\n","\n","                batch_time = time.time() - start_time\n","\n","                progress = (batch_idx + 1) / total_batches\n","                progress_bar = \">\" * int(30 * progress) + \"-\" * (30 - int(30 * progress))\n","                print(f\"\\r[{progress_bar}] {batch_idx + 1}/{total_batches} - Loss: {loss.item():.5f} - Batch Time: {batch_time:.3f} sec\", end='')\n","\n","                self.losses.reset()\n","\n","            epoch_time = time.time() - epoch_start_time\n","            if epoch > num_epoch - 10:\n","                self.save_model(epoch)\n","            training_loss /= len(training_loader)\n","            self.train_losses.append(training_loss)\n","            valid_loss = self.evaluate(self.model)\n","            self.valid_losses.append(valid_loss)\n","            scheduler1.step()\n","            scheduler2.step()\n","            scheduler3.step(valid_loss)\n","            print(f\" - Epoch Time: {epoch_time:.3f} sec - Train Loss: {training_loss:.5f} - Validation Loss: {valid_loss:.5f}\")\n","\n","model = XLAN()\n","optimizer = Optimizer(model)\n","dataloader = setup_loader(setup_dataset())\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","save_path = '/kaggle/working/model/model4'\n","\n","trainer = Trainer(model, optimizer, dataloader, device, save_path)\n","num_epoch = 100\n","trainer.train(num_epoch)\n","trainer.plot_losses()"]},{"cell_type":"code","execution_count":279,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T10:14:11.600037Z","iopub.status.busy":"2024-07-07T10:14:11.599635Z","iopub.status.idle":"2024-07-07T10:14:18.672427Z","shell.execute_reply":"2024-07-07T10:14:18.671504Z","shell.execute_reply.started":"2024-07-07T10:14:11.600007Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["113it [00:06, 17.18it/s]\n"]}],"source":["class Tester:\n","    def __init__(self, model, checkpoint, evaler, device, save_path):\n","        self.model = model\n","        self.checkpoint = checkpoint\n","        self.evaler = evaler\n","        self.device = device\n","        self.save_path = save_path\n","        \n","    def eval(self):\n","        self.model = torch.nn.DataParallel(self.model).to(self.device)\n","        self.model.load_state_dict(self.checkpoint)\n","        result = self.evaler(self.model, self.device)\n","        with open(self.save_path, \"w\") as json_file:\n","            json.dump(result, json_file)\n","            \n","#output_dir = '/kaggle/working/save_predict'\n","#os.makedirs(output_dir, exist_ok=True)\n","\n","model = XLAN()\n","checkpoint = torch.load('/kaggle/input/model-x-lan/Model/Model 1/model1_epoch_192.pth')\n","evaler = EvalTest(\n","            eval_ids = cfg.DATA_LOADER.TEST_ID,\n","            gv_feat = cfg.DATA_LOADER.TEST_GV_FEAT,\n","            att_feats = cfg.DATA_LOADER.TEST_ATT_FEATS)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","save_path = os.path.join(output_dir, 'caption_predictmodel1.json')\n","\n","tester = Tester(model, checkpoint, evaler, device, save_path)\n","tester.eval()"]},{"cell_type":"code","execution_count":280,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T10:14:25.626065Z","iopub.status.busy":"2024-07-07T10:14:25.625727Z","iopub.status.idle":"2024-07-07T10:14:25.634461Z","shell.execute_reply":"2024-07-07T10:14:25.633402Z","shell.execute_reply.started":"2024-07-07T10:14:25.626039Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["113 113\n"]}],"source":["import json\n","\n","actual = []\n","predict = []\n","\n","with open('/kaggle/working/save_predict/caption_predictmodel1.json', 'r') as f:\n","    results = json.load(f)\n","\n","for result in results:\n","    image_filename = result[cfg.INFERENCE.ID_KEY]\n","    caption_pred = result[cfg.INFERENCE.CAP_KEY]\n","    caption_act = images_captions_dict[image_filename]\n","    predict.append(caption_pred)\n","    caption_act = remove_start_and_end_tokens(caption_act)\n","    actual.append(caption_act)\n","\n","print(len(predict), len(actual))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-07-07T09:07:45.796837Z","iopub.status.busy":"2024-07-07T09:07:45.795911Z","iopub.status.idle":"2024-07-07T09:07:51.850132Z","shell.execute_reply":"2024-07-07T09:07:51.848674Z","shell.execute_reply.started":"2024-07-07T09:07:45.796789Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import os\n","\n","i = 0\n","\n","for result in results:\n","    image_filename = result[cfg.INFERENCE.ID_KEY]\n","    image_path = os.path.join('/kaggle/input/indonesian-traffic-violation-on-motorcycle/image_crop/image crop/image', image_filename + '.png')\n","    image = mpimg.imread(image_path)\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.show() \n","    print(image_filename)\n","    print(f\"predict: {predict[i]}\")\n","    print(f\"actual: {actual[i]}\")\n","    print(\"----------------------\")\n","    i += 1"]},{"cell_type":"code","execution_count":281,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T10:14:28.040315Z","iopub.status.busy":"2024-07-07T10:14:28.039651Z","iopub.status.idle":"2024-07-07T10:14:28.129413Z","shell.execute_reply":"2024-07-07T10:14:28.128518Z","shell.execute_reply.started":"2024-07-07T10:14:28.040282Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU-1: 0.655956\n","BLEU-2: 0.563424\n","BLEU-3: 0.473858\n","BLEU-4: 0.395459\n","Average: 0.5221742257749868\n","ROUGE-L: 0.6511320508977976\n"]}],"source":["from nltk.translate.bleu_score import corpus_bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","from nltk.util import ngrams\n","from nltk.metrics import *\n","from nltk.probability import FreqDist \n","\n","tokenized_predict = []\n","tokenized_actual = []\n","\n","for sentence in predict:\n","    tokens = sentence.split()\n","    tokenized_predict.append(tokens)\n","\n","for sentence in actual:\n","    tokens = sentence.split()\n","    tokenized_actual.append(tokens)\n","\n","actual_list = [[act] for act in tokenized_actual]\n","\n","bleu1_score = corpus_bleu(actual_list, tokenized_predict, weights=(1.0, 0, 0, 0))\n","print(\"BLEU-1: %f\" % bleu1_score)\n","bleu2_score = corpus_bleu(actual_list, tokenized_predict, weights=(0.5, 0.5, 0, 0), smoothing_function=SmoothingFunction().method1)\n","print(\"BLEU-2: %f\" % bleu2_score)\n","bleu3_score = corpus_bleu(actual_list, tokenized_predict, weights=(0.33, 0.33, 0.33, 0), smoothing_function=SmoothingFunction().method1)\n","print(\"BLEU-3: %f\" % bleu3_score)\n","bleu4_score = corpus_bleu(actual_list, tokenized_predict, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method1)\n","print(\"BLEU-4: %f\" % bleu4_score)\n","\n","avg_bleu = (bleu1_score+bleu2_score+bleu3_score+bleu4_score)/4\n","print(f\"Average: {avg_bleu}\")\n","\n","def lcsseq(x, y):\n","    m = len(x)\n","    n = len(y)\n","    dp = [[\"\"] * (n + 1) for _ in range(m + 1)]\n","\n","    for i in range(1, m + 1):\n","        for j in range(1, n + 1):\n","            if x[i - 1] == y[j - 1]:\n","                dp[i][j] = dp[i - 1][j - 1] + \" \" + x[i - 1]\n","            else:\n","                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1], key=len)\n","\n","    return dp[m][n].split()\n","\n","def rouge_l(evaluated, reference):\n","    evaluated_tokens = evaluated.split()\n","    reference_tokens = reference.split()\n","    lcs = lcsseq(evaluated_tokens, reference_tokens)\n","    lcs_length = len(lcs)\n","\n","    reference_length = len(reference_tokens)\n","\n","    if reference_length == 0:\n","        return 0\n","    else:\n","        return lcs_length / reference_length\n","\n","\n","rouge_l_scores = []\n","\n","for p, a in zip(predict, actual):\n","    rouge_l_scores.append(rouge_l(p, a))\n","\n","avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n","\n","print(f\"ROUGE-L: {avg_rouge_l}\")"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2024-07-07T09:30:22.901547Z","iopub.status.busy":"2024-07-07T09:30:22.900952Z","iopub.status.idle":"2024-07-07T09:30:22.932631Z","shell.execute_reply":"2024-07-07T09:30:22.931777Z","shell.execute_reply.started":"2024-07-07T09:30:22.901498Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average ROUGE-1 score: 0.6554760699997555\n","Average ROUGE-L score: 0.6271321670983303\n"]}],"source":["from nltk.util import ngrams\n","from nltk.metrics import *\n","from nltk.probability import FreqDist \n","\n","def lcsseq(x, y):\n","    m = len(x)\n","    n = len(y)\n","    dp = [[\"\"] * (n + 1) for _ in range(m + 1)]\n","\n","    for i in range(1, m + 1):\n","        for j in range(1, n + 1):\n","            if x[i - 1] == y[j - 1]:\n","                dp[i][j] = dp[i - 1][j - 1] + \" \" + x[i - 1]\n","            else:\n","                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1], key=len)\n","\n","    return dp[m][n].split()\n","\n","def rouge_l(evaluated, reference):\n","    evaluated_tokens = evaluated.split()\n","    reference_tokens = reference.split()\n","    lcs = lcsseq(evaluated_tokens, reference_tokens)\n","    lcs_length = len(lcs)\n","\n","    reference_length = len(reference_tokens)\n","\n","    if reference_length == 0:\n","        return 0\n","    else:\n","        return lcs_length / reference_length\n","\n","\n","rouge_l_scores = []\n","\n","for p, a in zip(predict, actual):\n","    rouge_l_scores.append(rouge_l(p, a))\n","\n","avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n","\n","print(f\"Average ROUGE-L score: {avg_rouge_l}\")\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5306564,"sourceId":8854084,"sourceType":"datasetVersion"},{"datasetId":5335586,"sourceId":8864609,"sourceType":"datasetVersion"},{"datasetId":5336453,"sourceId":8866799,"sourceType":"datasetVersion"},{"datasetId":5347989,"sourceId":8892725,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
